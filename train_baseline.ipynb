{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from segmentation_models_pytorch.utils.meter import AverageValueMeter\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.image_loading import load_training_images, load_groundtruth_images, load_test_images\n",
    "from custom_datasets import Sat_Mask_Dataset_UPP_preprocessed\n",
    "import albumentations as album\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "from torcheval.metrics.functional import multiclass_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "preprocess_input = get_preprocessing_fn('efficientnet-b5', pretrained='imagenet')\n",
    "preprocess_input =  album.Compose([album.Lambda(image=preprocess_input), ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "original_data = {}\n",
    "original_data[\"images\"] = load_training_images()\n",
    "original_data[\"masks\"] = load_groundtruth_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom data for specified cities\n",
    "city_names = [\"boston\", \"nyc\", \"philadelphia\", \"austin\"]\n",
    "custom_data = {\"images\": [], \"masks\": []}\n",
    "for name in city_names:\n",
    "    custom_data[\"images\"].extend(load_training_images(name))\n",
    "    custom_data[\"masks\"].extend(load_groundtruth_images(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training datasets\n",
    "custom_data_set = Sat_Mask_Dataset_UPP_preprocessed(custom_data[\"images\"], custom_data[\"masks\"], min_street_ratio=0.03, max_street_ratio=1.0, upp_preprocess=preprocess_input)\n",
    "original_data_set = Sat_Mask_Dataset_UPP_preprocessed(original_data[\"images\"], original_data[\"masks\"],  upp_preprocess=preprocess_input)\n",
    "print(\"After cleanup, the dataset now contains\", len(custom_data_set), \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Images in different styles:\n",
    "- Normal Satellite Image\n",
    "- Corresponding Mask\n",
    "- Preprocessed Satellite Image\n",
    "- Preprocessed Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(original_data[\"images\"][0].shape)\n",
    "plt.imshow(original_data[\"images\"][0])\n",
    "plt.show()\n",
    "print(original_data[\"masks\"][0].shape)\n",
    "plt.imshow(original_data[\"masks\"][0])\n",
    "plt.show()\n",
    "item = original_data_set.__getitem__(0)\n",
    "img_normal = np.swapaxes(item[0], 0, 2)\n",
    "img_mask = np.swapaxes(item[1], 0, 2)\n",
    "plt.imshow(img_normal)\n",
    "plt.show()\n",
    "plt.imshow(img_mask.squeeze(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split custom dataset into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(custom_data_set))\n",
    "valid_size = len(custom_data_set) - train_size\n",
    "train_dataset, valid_dataset = random_split(custom_data_set, [train_size, valid_size])\n",
    "\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models that are used (ONLY RUN MODEL THAT SHOULD BE USED):\n",
    "\n",
    "Please note that each model needs to be run separately. Hence one needs to run the notebook 4 times, each time with the correct model version selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=(1024,512,256,64,16),\n",
    "    decoder_attention_type=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None\n",
    ").to(device)\n",
    "\n",
    "model_name = \"unet_resnet34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=(1024,512,256,64,16),\n",
    "    decoder_attention_type=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None\n",
    ").to(device)\n",
    "\n",
    "model_name = \"unet_efficientnet-b5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=(1024,512,256,64,16),\n",
    "    decoder_attention_type=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None\n",
    ").to(device)\n",
    "\n",
    "model_name = \"unetplusplus_resnet34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    decoder_use_batchnorm=True,\n",
    "    decoder_channels=(1024,512,256,64,16),\n",
    "    decoder_attention_type=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    aux_params=None\n",
    ").to(device)\n",
    "\n",
    "model_name = \"unetplusplus_efficientnet-b5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some facts about the model in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model loaded\")\n",
    "# how many trainable parameters does the model have?\n",
    "print(\"Trainable parameters\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# how many total parameters does the model have?\n",
    "print(\"Total parameters\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# percentage of trainable parameters\n",
    "print(\"Percentage of trainable parameters: \")\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1./0.13]).to(device)\n",
    "bce_loss = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "soft_bce_loss = smp.losses.SoftBCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "def dice_loss(logits,masks, smooth=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs_flat = probs.reshape(-1)\n",
    "    masks_flat = masks.reshape(-1)\n",
    "    \n",
    "    intersection = (probs_flat * masks_flat).sum()\n",
    "    union = probs_flat.sum() + masks_flat.sum()\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_coeff\n",
    "\n",
    "def combined_loss_correct_dice(logits, masks, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size, 416*416))\n",
    "    mask_sq = masks.reshape((batch_size, 416*416))\n",
    "    \n",
    "    return dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_f1_score_from_logits(pred, mask):\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_f1_score_from_classes(mask, pred_classes)\n",
    "\n",
    "def mean_f1_score_from_classes(preds, masks):\n",
    "    b_size = masks.shape[0]\n",
    "    f1_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i]\n",
    "        pred = preds[i]\n",
    "        f1_acc = f1_acc + multiclass_f1_score(pred.reshape((size)), mask.reshape((size)))\n",
    "    mean_f1 = f1_acc/b_size\n",
    "    return mean_f1\n",
    "\n",
    "def mean_iou_from_logits(pred, mask):\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_iou_from_classes(mask, pred_classes)\n",
    "\n",
    "def mean_iou_from_classes(preds, masks):\n",
    "    b_size = masks.shape[0]\n",
    "    iou_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i].reshape((size)).bool()\n",
    "        pred = preds[i].reshape((size)).bool()\n",
    "        intersection = (pred & mask).float().sum()\n",
    "        union = (pred | mask).float().sum()\n",
    "        iou = (intersection + 1e-10) / (union + 1e-10)\n",
    "        iou_acc = iou_acc + iou\n",
    "    mean_iou = iou_acc / b_size\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0005)])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=10, verbose=True, threshold=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If needed, load the model from a specific model checkpoint by uncommenting the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"model/best_model_0.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_scores = []\n",
    "f1_scores = []\n",
    "iou_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we train on the custom dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    average_loss = 0\n",
    "    f1_score = 0\n",
    "    iou_score = 0\n",
    "\n",
    "    for data in tqdm.tqdm(train_loader, leave=False):\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(x)\n",
    "\n",
    "        loss = combined_loss_correct_dice(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss, f1 and iou\n",
    "        average_loss += loss.item()\n",
    "        f1_score += mean_f1_score_from_logits(y_pred, y)\n",
    "        iou_score += mean_iou_from_logits(y_pred, y)\n",
    "\n",
    "    # Print training stats for the current epoch\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {average_loss / len(train_loader)}, F1: {f1_score / len(train_loader)}, IOU: {iou_score / len(train_loader)}\")\n",
    "\n",
    "\n",
    "    # store the model after each epoch\n",
    "    torch.save(model.state_dict(), f\"model/best_model_{epoch}.pth\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    average_loss = 0\n",
    "    f1_score = 0\n",
    "    iou_score = 0\n",
    "\n",
    "    for x, y in tqdm.tqdm(valid_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model.forward(x)\n",
    "            loss = combined_loss_correct_dice(y_pred, y)\n",
    "\n",
    "        # Update loss, f1 and iou\n",
    "        average_loss += loss.item()\n",
    "        f1_score += mean_f1_score_from_logits(y_pred, y)\n",
    "        iou_score += mean_iou_from_logits(y_pred, y)\n",
    "\n",
    "    # Print validation stats for the current epoch\n",
    "    val_loss = average_loss / len(valid_loader)\n",
    "    val_f1 = f1_score / len(valid_loader)\n",
    "    val_iou = iou_score / len(valid_loader)\n",
    "    print(f\"Epoch: {epoch}, Validation Loss: {val_loss}, F1: {val_f1}, IOU: {val_iou}\")\n",
    "    loss_scores.append(val_loss)\n",
    "    f1_scores.append(val_f1.cpu().item())\n",
    "    iou_scores.append(val_f1.cpu().item())\n",
    "    \n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPP-Model, SoftBCELoss:\\\n",
    "The lowest training value is 0.8790411872128463 at index 12.\\\n",
    "The largest f1-score is 0.9565524458885193 at index 44.\\\n",
    "The largest iou-score is 0.7358250021934509 at index 27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a visual representation of the intermediate training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "item = valid_loader.dataset[0]\n",
    "image_visual = item[0]\n",
    "mask_visual = item[1]\n",
    "print(image_visual.shape)\n",
    "print(mask_visual.shape)\n",
    "img_normal = np.swapaxes(image_visual, 0, 2)\n",
    "img_mask = np.swapaxes(mask_visual, 0, 2)\n",
    "plt.imshow(img_normal)\n",
    "plt.show()\n",
    "plt.imshow(img_mask)\n",
    "plt.show()\n",
    "model.eval()\n",
    "pred = None\n",
    "with torch.no_grad():\n",
    "    image = image_visual.unsqueeze(0).to(device)\n",
    "    pred = model(image).cpu()\n",
    "plt.imshow(pred.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(torch.round(torch.sigmoid(pred.squeeze())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If needed, load the model from a specific model checkpoint by uncommenting the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"model_XXXX.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split original Kaggle dataset into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(original_data_set))\n",
    "valid_size = len(original_data_set) - train_size\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0)\n",
    "train_dataset, valid_dataset = random_split(original_data_set, [train_size, valid_size], generator=gen)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_loader))\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    average_loss = 0\n",
    "    f1_score = 0\n",
    "    iou_score = 0\n",
    "\n",
    "    for data in tqdm.tqdm(train_loader, leave=False):\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.forward(x)\n",
    "\n",
    "        loss = soft_bce_loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss, f1 and iou\n",
    "        average_loss += loss.item()\n",
    "        f1_score += mean_f1_score_from_logits(y_pred, y)\n",
    "        iou_score += mean_iou_from_logits(y_pred, y)\n",
    "\n",
    "    # Print training stats for the current epoch\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {average_loss / len(train_loader)}, F1: {f1_score / len(train_loader)}, IOU: {iou_score / len(train_loader)}\")\n",
    "\n",
    "\n",
    "    # store the model after each epoch \n",
    "    torch.save(model.state_dict(), f\"model/{model_name}.pth\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    average_loss = 0\n",
    "    f1_score = 0\n",
    "    iou_score = 0\n",
    "\n",
    "    for x, y in tqdm.tqdm(valid_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model.forward(x)\n",
    "            loss = soft_bce_loss(y_pred, y)\n",
    "\n",
    "        # Update loss, f1 and iou\n",
    "        average_loss += loss.item()\n",
    "        f1_score += mean_f1_score_from_logits(y_pred, y)\n",
    "        iou_score += mean_iou_from_logits(y_pred, y)\n",
    "\n",
    "    # Print validation stats for the current epoch\n",
    "    val_loss = average_loss / len(valid_loader)\n",
    "    val_f1 = f1_score / len(valid_loader)\n",
    "    val_iou = iou_score / len(valid_loader)\n",
    "    print(f\"Epoch: {epoch}, Validation Loss: {val_loss}, F1: {val_f1}, IOU: {val_iou}\")\n",
    "    loss_scores.append(val_loss)\n",
    "    f1_scores.append(val_f1.cpu().item())\n",
    "    iou_scores.append(val_f1.cpu().item())\n",
    "    \n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines (F1-score):\n",
    "- Unet Efficient: 0.9400148987770081 (Validation), 0.93428 (Kaggle)\n",
    "- UPP Efficient: 0.936101496219635 (Validation), 0.93273 (Kaggle)\n",
    "- Unet ResNet: 0.9320721626281738 (Validation), 0.92591 (Kaggle)\n",
    "- UPP ResNet: 0.9340260028839111 (Validation), 0.92733 (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Kaggle Submission:\n",
    "\n",
    "Now we go over each trained model and generate a submission file for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets import Sat_Only_Image_UPP_preprocessed\n",
    "kaggle_submission_images = load_test_images()\n",
    "submission_data_set = Sat_Only_Image_UPP_preprocessed(kaggle_submission_images, upp_preprocess=preprocess_input)\n",
    "submission_dataloader = DataLoader(submission_data_set, batch_size=1, shuffle=False, drop_last=False, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from mask_to_submission import make_submission\n",
    "\n",
    "\n",
    "model_names = [\"unet_resnet34\", \"unet_efficientnet-b5\", \"unetplusplus_resnet34\", \"unetplusplus_efficientnet-b5\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    # construct the model depending if name is unet or unetplusplus and efficientnet or resnet34\n",
    "    encoder_name = model_name.split(\"_\")[1]\n",
    "    model_type = model_name.split(\"_\")[0]\n",
    "    if model_type == \"unet\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_depth=5,\n",
    "            encoder_weights=\"imagenet\",\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(1024,512,256,64,16),\n",
    "            decoder_attention_type=None,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "            aux_params=None\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = smp.UnetPlusPlus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_depth=5,\n",
    "            encoder_weights=\"imagenet\",\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(1024,512,256,64,16),\n",
    "            decoder_attention_type=None,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "            aux_params=None\n",
    "        ).to(device)\n",
    "\n",
    "    # load the model weights from the training\n",
    "    model.load_state_dict(torch.load(f\"model/{model_name}.pth\"))\n",
    "\n",
    "    # make the predictions for each model. \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm.tqdm(submission_dataloader):\n",
    "            image = image.to(device)\n",
    "            pred = model(image).cpu()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    counter = 144\n",
    "    path = \"submission/model/\"\n",
    "    final_pred_images = []\n",
    "    for sp in predictions:\n",
    "        pred_image = torch.round(torch.sigmoid(sp.squeeze()))\n",
    "        pred = F.interpolate(pred_image.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "        pred = pred.squeeze().numpy()\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        pred = np.stack([pred, pred, pred],axis=2)\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", pred)\n",
    "        counter += 1\n",
    "        final_pred_images.append(pred)\n",
    "\n",
    "    make_submission(f\"{model_name}.csv\", path, foreground_threshold= 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

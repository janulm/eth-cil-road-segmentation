{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiSeSAM - Fine-tuning SAM on Satellite Images for Street Segmentation\n",
    "\n",
    "Our approach is based on harnessing the power of pretrained ViT image encoder from META's Segment Anything Model SAM.\n",
    "We explore a combination of different custom decoders, we name this model architecture BiSeSAM. BiSeSAM will be finetuned on Satellite Images for Street Segmentation. (BiSeSAM - Binary Semantic SAM)\n",
    "\n",
    "We will try to use different approaches for the Decocer: \n",
    "\n",
    "1. Conv/Deconv based approach\n",
    "2. Fully connected MLP's\n",
    "3. Skip Connection MLP\n",
    "4. Spatially Aware MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Jannek Ulm 16.5.2024\n",
    "# code was inspired by the following sources: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
    "\n",
    "from utils.image_loading import * \n",
    "from utils.torch_device import *\n",
    "from custom_datasets import Sat_Mask_Dataset, Sat_Only_Image_Dataset\n",
    "\n",
    "device = get_torch_device(allow_mps=True)\n",
    "print(\"using device:\",device)\n",
    "\n",
    "###### KAGGLE DATA SET #####\n",
    "kaggle_data = {}\n",
    "kaggle_data[\"images\"] =load_training_images()\n",
    "kaggle_data[\"masks\"] = load_groundtruth_images()\n",
    "\n",
    "######## EPFL DATA SET #####\n",
    "epfl_data = {}\n",
    "epfl_data[\"images\"] = load_training_images(\"EPFL\")\n",
    "epfl_data[\"masks\"] = load_groundtruth_images(\"EPFL\")\n",
    "\n",
    "\n",
    "####### CUSTOM GOOGLE MAPS DATA SET #####\n",
    "city_names = [\"boston\",\"nyc\",\"philadelphia\",\"austin\"]\n",
    "gmaps_data = {\"images\":[],\"masks\":[]} # stores images and gt masks\n",
    "for name in city_names:\n",
    "    gmaps_data[\"images\"].extend(load_training_images(name))\n",
    "    gmaps_data[\"masks\"].extend(load_groundtruth_images(name))\n",
    "assert (len(gmaps_data[\"images\"]) == len(gmaps_data[\"masks\"]))\n",
    "\n",
    "\n",
    "######## PRINT GOOGLE MAPS DS STATISTICS ########\n",
    "print(\"the raw custom dataset contains\",len(gmaps_data[\"images\"]),\"images\")\n",
    "print(\"custom ds: (min,mean,max) street ratio\",get_street_ratio_mmm(gmaps_data[\"masks\"]))\n",
    "print(\"orig ds: (min,mean,max) street ratio\",get_street_ratio_mmm(kaggle_data[\"masks\"]))\n",
    "print(\"custom ds with ignore under threshold: (min,mean,max) street ratio\",get_street_ratio_mmm(gmaps_data[\"masks\"],min_ratio_threshold=0.03))\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "gmaps_data_set = Sat_Mask_Dataset(gmaps_data[\"images\"], gmaps_data[\"masks\"],min_street_ratio=0.03,max_street_ratio=1.0)\n",
    "kaggle_data_set  = Sat_Mask_Dataset(kaggle_data[\"images\"],kaggle_data[\"masks\"])\n",
    "epfl_data_set = Sat_Mask_Dataset(epfl_data[\"images\"],epfl_data[\"masks\"])\n",
    "\n",
    "print(\"after cleanup, the dataset now contains\",len(kaggle_data_set),\"images\")\n",
    "\n",
    "\n",
    "######## KAGGLE - SUBMISSION DATA SET ########\n",
    "kaggle_submission_images = load_test_images()\n",
    "kaggle_submission_data_set = Sat_Only_Image_Dataset(kaggle_submission_images)\n",
    "\n",
    "\n",
    "\n",
    "# print num of data in original and custom dataset\n",
    "print(\"original dataset contains\",len(gmaps_data_set),\"images\")\n",
    "print(\"custom dataset contains\",len(kaggle_data_set),\"images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_images = True\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if show_sample_images:\n",
    "    \n",
    "    ######### show distribution of street ratio:\n",
    "    \n",
    "    # plot the street ratio distribution of the dataset\n",
    "\n",
    "    custom_ratios = get_street_ratio_distr(gmaps_data[\"masks\"])\n",
    "    original_ratios = get_street_ratio_distr(kaggle_data[\"masks\"])\n",
    "\n",
    "    plt.hist(custom_ratios,40)\n",
    "    plt.hist(original_ratios,40)\n",
    "    plt.show()\n",
    "    \n",
    "    # get some random training images\n",
    "    idx = 4\n",
    "    image, mask = kaggle_data_set[idx]\n",
    "\n",
    "    img = np.array(image).astype(np.uint8)\n",
    "    # swap first and third dimension\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    mask = np.array(mask)\n",
    "    mask = np.swapaxes(mask, 0, 2)\n",
    "\n",
    "    # Create a reddish tone image with the same shape as the original image\n",
    "    reddish_tone = np.zeros_like(img)\n",
    "    reddish_tone[..., 0] = 255  # Red channel to maximum\n",
    "\n",
    "    # Define the opacity for the overlay\n",
    "    opacity = 0.5\n",
    "\n",
    "    # Ensure the mask is binary (0 or 255) and has the same shape as the original image\n",
    "    binary_mask = (mask > 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Blend the original image and the reddish tone based on the mask\n",
    "    blended_image = np.where(binary_mask == 255, \n",
    "                            (img * (1 - opacity) + reddish_tone * opacity).astype(np.uint8),\n",
    "                            img)\n",
    "\n",
    "    # Display the original image, mask, and blended image side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(binary_mask, cmap='gray')\n",
    "    axes[1].set_title('Mask')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(blended_image)\n",
    "    axes[2].set_title('Image with Overlay')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "\n",
    "\n",
    "gpu_batch_size = 1\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "torch.manual_seed(0)\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0)\n",
    "\n",
    "\n",
    "kaggle_train_dataset, kaggle_test_dataset = torch.utils.data.random_split(kaggle_data_set, [train_split, 1-train_split],generator=gen)\n",
    "gmaps_train_dataset, gmaps_test_dataset = torch.utils.data.random_split(gmaps_data_set, [train_split, 1-train_split],generator=gen)\n",
    "\n",
    "\n",
    "# combine kaggle and epfl data set\n",
    "kaggle_epfl_dataset = torch.utils.data.ConcatDataset([kaggle_data_set,epfl_data_set])\n",
    "# split the combined dataset into train and test set\n",
    "kaggle_epfl_train_dataset, kaggle_epfl_test_dataset = torch.utils.data.random_split(kaggle_epfl_dataset, [train_split, 1-train_split],generator=gen)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "gmaps_dataloader = DataLoader(gmaps_data_set, batch_size=1, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# kaggle dataloader\n",
    "kaggle_dataloader = DataLoader(kaggle_data_set, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "kaggle_train_dataloader = DataLoader(kaggle_train_dataset, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "kaggle_test_dataloader = DataLoader(kaggle_test_dataset, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# kaggel_epfl dataloader\n",
    "kaggle_epfl_train_dataloader = DataLoader(kaggle_epfl_train_dataset, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "kaggle_epfl_test_dataloader = DataLoader(kaggle_epfl_test_dataset, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "\n",
    "# dataloader for submission dataset: \n",
    "submission_dataloader = DataLoader(kaggle_submission_data_set, batch_size=1, shuffle=False, drop_last=False,num_workers=4,persistent_workers=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ (CUSTOM SAM (stored in repo))\n",
    "from custom_segment_anything.segment_anything import sam_model_registry\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import *\n",
    "\n",
    "# load the model from checkpoints on disk:\n",
    "def load_sam_decoder_model_from_checkpoint(encoder_option:int, device,decoder):\n",
    "    #\n",
    "    # encoder_option =0 : for vit_b with the encoder that just retunrs the final block output.\n",
    "    # encoder_option =1 : for vit_b with the encoder that returns the intermediate outputs + final output.  \n",
    "    sam_checkpoint_path = \"custom_segment_anything/model_checkpoints/\"\n",
    "    # base, large, huge checkpoints. \n",
    "    checkpoint_names = [\"vit_b\",\"vit_b_intermediate\"]#,\"vit_l\",\"vit_h\"]\n",
    "    checkpoints = [\"sam_vit_b_01ec64.pth\"] #,\"sam_vit_l_0b3195.pth\",\"sam_vit_h_4b8939.pth\"]\n",
    "    model_paths = [sam_checkpoint_path+checkpoint_name for checkpoint_name in checkpoints]\n",
    "    sam = sam_model_registry[checkpoint_names[encoder_option]](checkpoint=model_paths[0])\n",
    "    sam.to(device)\n",
    "    model = SAM_Encoder_Custom_Decoder(sam.preprocess, sam.image_encoder,decoder=decoder)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the finetnued model state, if already started training. \n",
    "def load_finetuned_model(name,device,decoder):\n",
    "    model = load_sam_decoder_model_from_checkpoint(0,device,decoder)\n",
    "    finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "    model.load_state_dict(torch.load(finetune_path+name,map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "############################\n",
    "\n",
    "def load_sam_model(decoder_option, device, encoder_finetune_num_first_layers,encoder_finetune_num_last_layers,finetuned_model_name=\"model.pth\",sam_checkpoint_or_finetuned=\"sam\",load_from_compiled=False):\n",
    "    #   \n",
    "    #   decoder_options: [\"conv\", \"mlp\" , \"spatial-full\", \"spatial-small\", \"skip-connect]\n",
    "    #   encoder_finetune_num_last_layers tells how many layers of sam encoder are finetuned, all decoder layers are tuned. \n",
    "    #   sam_checkpoint_or_finetuned checks if the model is loaded from a sam checkpoint or a finetuned model with the same architecture.\n",
    "    #\n",
    "    # first construct the model from sam_checkpoint:\n",
    "\n",
    "    if decoder_option == \"conv\":\n",
    "        decoder = Conv_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder)\n",
    "   \n",
    "    elif decoder_option == \"mlp\":\n",
    "        decoder = MLP_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder)\n",
    "\n",
    "    elif decoder_option == \"spatial-full\":\n",
    "        decoder = MLP_Decoder_Spatially_Aware(context_option=1)\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder)\n",
    "\n",
    "    elif decoder_option == \"spatial-small\":\n",
    "        decoder = MLP_Decoder_Spatially_Aware(context_option=0)\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder)\n",
    "    elif decoder_option == \"skip-connect\":\n",
    "        decoder = Skip_MLP_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(1,device,decoder)\n",
    "    else:\n",
    "        raise ValueError(\"invalid decoder option\")\n",
    "    \n",
    "    # if should load from fine-tuned model, load the model from the finetuned path.\n",
    "    if sam_checkpoint_or_finetuned == \"finetuned\":\n",
    "        finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "        \n",
    "        if \"custom_segment_anything/model_checkpoints\" in finetuned_model_name:\n",
    "            load_path = finetuned_model_name+\".pth\"\n",
    "        else:\n",
    "            load_path = finetune_path+finetuned_model_name+\".pth\"\n",
    "        if load_from_compiled:\n",
    "            model = torch.compile(model)\n",
    "        model.load_state_dict(torch.load(load_path,map_location=torch.device('cpu')))\n",
    "    elif sam_checkpoint_or_finetuned == \"sam\":\n",
    "        pass\n",
    "        # already initialized model from sam_checkpoint\n",
    "    else: \n",
    "        raise ValueError(\"invalid sam_checkpoint_or_finetuned option\")\n",
    "    \n",
    "    \n",
    "    # Unfreeze last layers of the encoder\n",
    "    for layer_number, param in enumerate(model.sam_encoder.parameters()):\n",
    "        if layer_number > 176 - encoder_finetune_num_last_layers or layer_number < encoder_finetune_num_first_layers:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze neck of the encoder\n",
    "    model.sam_encoder.neck.requires_grad = True\n",
    "    #model.requires_grad = True\n",
    "    print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "def mean_f1_score_from_logits(pred,mask):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # compute the mean for all the images\n",
    "    # computes the mean over the 0-th axis\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_f1_score_from_classes(mask,pred_classes)\n",
    "\n",
    "\n",
    "def mean_f1_score_from_classes(preds,masks):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # this computes the f1 over the whole batch, for each image in the batch alone:\n",
    "    # first reshape the tensors\n",
    "    b_size = masks.shape[0]\n",
    "    f1_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i]\n",
    "        pred = preds[i]\n",
    "        # reshape and compute f1\n",
    "        f1_acc = f1_acc + multiclass_f1_score(pred.reshape((size)),mask.reshape((size)))\n",
    "        \n",
    "    mean_f1 = f1_acc/b_size\n",
    "    return mean_f1\n",
    "\n",
    "def dice_loss(logits,masks, smooth=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs_flat = probs.reshape(-1)\n",
    "    masks_flat = masks.reshape(-1)\n",
    "    \n",
    "    intersection = (probs_flat * masks_flat).sum()\n",
    "    union = probs_flat.sum() + masks_flat.sum()\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_coeff\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([1./0.13]).to(device)  # Example weights: adjust based on your dataset\n",
    "bce_loss = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "def focal_loss(logits, masks, alpha=0.15, gamma=2.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    loss = sigmoid_focal_loss(probs, masks, alpha=alpha, gamma=gamma, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def only_bce(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = masks.reshape((batch_size,1024*1024))\n",
    "    return bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_dice(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = masks.reshape((batch_size,1024*1024))\n",
    "    return dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - All Decoder Variants\n",
    "go over all possible decoder models\n",
    "\n",
    "train for a few epochs with only few encoder layers unlocked until the last epoch didnt improve the original loss\n",
    "increase the number of layers finetuned, \n",
    "if the whole epoch didnt improve, then stop the training in general\n",
    "\n",
    "always store the model, start each new layer round with the best of the last or second last stored model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEAR ALL CUDA MEMORY\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "train_loader = gmaps_dataloader\n",
    "test_loader = kaggle_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader)\n",
    "20\n",
    "do_intermed_prints = False\n",
    "\n",
    "#########\n",
    "\n",
    "decoder_options =  [\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\",\"mlp\"]\n",
    "num_layers_to_finetune_back = [25,65,85,105]\n",
    "num_layers_to_finetune_front = [0,0,15,25]\n",
    "learning_rates = [0.001,0.0001, 0.0001,0.0001]\n",
    "loss_functions = [only_bce,combined_loss_dice,combined_loss_dice]\n",
    "max_epochs = [3,3,4,4]\n",
    "grad_batch_size_choices = [5,5,5,5]\n",
    "\n",
    "##################################\n",
    "# TRAINING LOOP\n",
    "################################\n",
    "\n",
    "# OPTIMIZATIONS \n",
    "mean_f1_score_from_logits = torch.compile(mean_f1_score_from_logits)\n",
    "\n",
    "loss_fn_idx = 2\n",
    "# Hyperparameter tuning yielded that we use the loss function BCE + Dice for the best results\n",
    "\n",
    "loss_fn = loss_functions[loss_fn_idx]\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "print(\"using loss function:\",loss_fn)\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "        \n",
    "    for idx_layer_option in range(len(num_layers_to_finetune_back)):\n",
    "        # set the max number of epochs for this layer option\n",
    "        max_num_epochs = max_epochs[idx_layer_option]\n",
    "        \n",
    "        for epoch_counter in range(max_num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            epoch_to_train = epoch_counter\n",
    "            layer_option_back = num_layers_to_finetune_back[idx_layer_option]\n",
    "            layer_option_front = num_layers_to_finetune_front[idx_layer_option]\n",
    "            \n",
    "            # if the layer_option is larger than 1 (i.e. if at least 85 layers in the back are tuned, reduce lr.), reduce lr \n",
    "            learning_rate = learning_rates[idx_layer_option]\n",
    "            if idx_layer_option >= 2:\n",
    "                learning_rate = learning_rate / (epoch_counter + 1)\n",
    "                print(\"divided lr\")\n",
    "            print(\"using lr:\",learning_rate)\n",
    "            #loss_fn = loss_functions[loss_fn_idx]\n",
    "            #print(\"using loss function:\",loss_fn)\n",
    "            grad_batch_size = grad_batch_size_choices[loss_fn_idx]\n",
    "            \n",
    "            #####################################            \n",
    "            # now training this model \n",
    "            current_model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option_front}_{layer_option_back}_epoch_{epoch_counter}\"\n",
    "            print(\"training model:\",current_model_description)\n",
    "            # check if this current model description already exists, if so, load the model and skip this exact training step:\n",
    "            if os.path.exists(\"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\"):\n",
    "                print(\"model already exists, skipping training for this layer option\")\n",
    "                continue\n",
    "            # since model was not already trained\n",
    "            # load the \"start model from checkpoint or finetuned\"\n",
    "            # load the initial model from the sam checkpoint\n",
    "            if idx_layer_option == 0 and epoch_counter == 0:\n",
    "                print(\"loading model from sam checkpoint\")\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=layer_option_front,encoder_finetune_num_last_layers=layer_option_back,finetuned_model_name=None,sam_checkpoint_or_finetuned=\"sam\",load_from_compiled=False)\n",
    "                model = torch.compile(model)\n",
    "            else:\n",
    "                \n",
    "                if epoch_counter == 0:\n",
    "                    last_max_epoch = max_epochs[idx_layer_option-1]\n",
    "                    # now epoch 0, hence load max epoch from previous layer option\n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{num_layers_to_finetune_front[idx_layer_option-1]}_{num_layers_to_finetune_back[idx_layer_option-1]}_epoch_{last_max_epoch-1}\"\n",
    "                else:\n",
    "                    # load the last epoch from current layer option\n",
    "                    \n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option_front}_{layer_option_back}_epoch_{epoch_counter-1}\"\n",
    "                print(\"loading model from finetuned:\",model_description)\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=layer_option_front,encoder_finetune_num_last_layers=layer_option_back,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "                \n",
    "            # newly initializing the optimizer and scheduler since model was loaded new (do this for every epoch:)\n",
    "            ####################################\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            print(\"Starting Epoch: \",epoch_counter)\n",
    "            # training run: \n",
    "            model.train()\n",
    "            # store running losses for the epoch and the 10% print interval\n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_f1 = 0.0\n",
    "\n",
    "            short_running_loss = 0.0\n",
    "            short_running_f1 = 0.0\n",
    "\n",
    "            step_counter = 0\n",
    "            \n",
    "            # reset the gradients: \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #########################################\n",
    "            # TRAINING LOOP\n",
    "            for image, mask in tqdm(train_loader):\n",
    "                step_counter += 1\n",
    "                #####################\n",
    "                # forward pass\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)    \n",
    "                pred = model(image)\n",
    "                # compute loss and f1 score: \n",
    "                loss = loss_fn(pred,mask)\n",
    "                \n",
    "                \n",
    "                f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                epoch_running_f1 += f1_score\n",
    "                short_running_loss += loss.item()\n",
    "                short_running_f1 += f1_score\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                ###############\n",
    "                # backward pass\n",
    "                if step_counter % grad_batch_size == 0:\n",
    "                    # update the model weights\n",
    "                    optimizer.step()\n",
    "                    # reset the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                if do_intermed_prints and step_counter % print_interval == 0:\n",
    "                    print(\"step: \",step_counter//print_interval)\n",
    "                    # print out the current losses:\n",
    "                    print(f\"Epoch: {epoch_counter}, step: {step_counter//print_interval}, (train) Loss: {short_running_loss/print_interval}, F1: {short_running_f1/print_interval}\")\n",
    "                    # and reset the short running losses\n",
    "                    short_running_loss = 0.0\n",
    "                    short_running_f1 = 0.0\n",
    "\n",
    "            print(f\"Epoch: {epoch_counter}, (train) Loss: {epoch_running_loss/len(train_loader)}, F1: {epoch_running_f1/len(train_loader)}\")\n",
    "            ########################################\n",
    "            # save the model in every epoch\n",
    "            print(\"saving model:\",current_model_description)\n",
    "            torch.save(model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\")\n",
    "            #########################################\n",
    "            \n",
    "            # testing run: \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                l_acc = 0.0\n",
    "                score_acc = 0.0\n",
    "                for image,mask in tqdm(test_loader):\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    score = mean_f1_score_from_logits(pred,mask)    \n",
    "                    loss =  loss_fn(pred,mask)\n",
    "                    # update running loss and f1 score\n",
    "                    score_acc += score.item()\n",
    "                    l_acc  += loss.item()\n",
    "                    # store the loss and f1 score\n",
    "                print(f\"Epoch: {epoch_counter}, (test) Loss: {l_acc/len(test_loader)}, F1-Score: {score_acc/len(test_loader)}\")\n",
    "        # save the model after the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pre-Training Performance\n",
    "\n",
    "Now, check the performance on the kaggle dataset of the last model trained for each decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kaggle_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "decoder_options =  [\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\",\"mlp\"]\n",
    "## select the dataloader\n",
    "test_loader = kaggle_dataloader\n",
    "\n",
    "losses = {}\n",
    "f1_scores = {}\n",
    "\n",
    "layers_front = [25] # [0,0,15,25]\n",
    "layers_back = [105] # [25,65,85,105]\n",
    "epoch_ranges = [4] # [3,3,4,4]\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "loss_fn_idx = 2\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    losses[decoder_option] = []\n",
    "    f1_scores[decoder_option] = []\n",
    "    \n",
    "    for layer_idx in range(len(layers_front)):\n",
    "        epoch_range = epoch_ranges[layer_idx]\n",
    "        for epoch_counter in range(epoch_range):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            l_front = layers_front[layer_idx]\n",
    "            l_back = layers_back[layer_idx]\n",
    "            \n",
    "            model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{l_front}_{l_back}_epoch_{epoch_counter}\"\n",
    "            model_path = \"custom_segment_anything/model_checkpoints/finetuned/\"+model_description\n",
    "            compiled = (decoder_option != \"mlp\") # only the mlp was trained with the non compiled option initially\n",
    "            model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=model_path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=compiled)\n",
    "            \n",
    "            # for each model compute the f1 and loss score of the original dataset\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                l_acc = 0.0\n",
    "                score_acc = 0.0\n",
    "                for image,mask in tqdm(test_loader):\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    score = mean_f1_score_from_logits(pred,mask)    \n",
    "                    loss =  loss_fn(pred,mask)\n",
    "                    # update running loss and f1 score\n",
    "                    score_acc += score.item()\n",
    "                    l_acc  += loss.item()\n",
    "                    # store the loss and f1 score\n",
    "                    \n",
    "                mean_f1 = score_acc/len(test_loader)\n",
    "                mean_loss = l_acc/len(test_loader)\n",
    "                losses[decoder_option].append(mean_loss)\n",
    "                f1_scores[decoder_option].append(mean_f1)\n",
    "                print(f\"Decoder: {decoder_option},Loss Function {loss_fn_idx}: Layer_idx: {layer_idx} Epoch: {epoch_counter}, (test) Loss: {mean_loss}, F1-Score: {mean_f1}\")\n",
    "            \n",
    "# plot the resolts for all loss function in two seperate plots, one for f1 and one for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_sum = sum(epoch_ranges)\n",
    "epochs = list(range(epoch_sum))\n",
    "\n",
    "# Plot F1 Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "for decoder_option in decoder_options:\n",
    "    plt.plot(epochs, f1_scores[decoder_option], label=decoder_option)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score over Epochs for different Decoders')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "for decoder_option in decoder_options:\n",
    "    plt.plot(epochs, losses[decoder_option], label=decoder_option)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('weighted BCE+Dice Loss')\n",
    "plt.title('Loss over Epochs for Different Loss Functions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out the best finetuning strategy:\n",
    "\n",
    "for this try out different learning rate and dataset options. Tune for at most 20 epochs, get the scores and validate them in the kaggle submission system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "lr_options = [0.0001,0.00001]\n",
    "\n",
    "training_options = [kaggle_train_dataloader,kaggle_epfl_train_dataloader]\n",
    "test_options = [kaggle_test_dataloader,kaggle_epfl_test_dataloader]\n",
    "\n",
    "training_desc = [\"kaggle\",\"kaggle_epfl\"]\n",
    "\n",
    "# now check only for the best model, the mlp model\n",
    "\n",
    "\n",
    "max_epochs = 21\n",
    "\n",
    "train_losses = {}\n",
    "train_f1_scores = {}\n",
    "test_losses = {}\n",
    "test_f1_scores = {}\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "\n",
    "for lr in lr_options:\n",
    "    for train_idx in range(len(training_desc)):\n",
    "        train_loader = training_options[train_idx]\n",
    "        test_loader = test_options[train_idx]\n",
    "        \n",
    "        desc_key = f\"lr_{lr}_{training_desc[train_idx]}\"\n",
    "        \n",
    "        print(\"now doing: \",desc_key)\n",
    "        \n",
    "        train_losses[desc_key] = []\n",
    "        train_f1_scores[desc_key] = []\n",
    "        test_losses[desc_key] = []\n",
    "        test_f1_scores[desc_key] = []\n",
    "        \n",
    "        model = load_sam_model(\"mlp\",device,encoder_finetune_num_first_layers=25,encoder_finetune_num_last_layers=105,finetuned_model_name=\"model_3_mlp_decoder_finetune_last_25_105_epoch_3\",sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=False)\n",
    "        model = torch.compile(model)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            # TRAIN\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_f1 = 0.0\n",
    "            step_counter = 0\n",
    "            for img,mask in tqdm(train_loader):\n",
    "                step_counter += 1\n",
    "                img = img.to(device)\n",
    "                mask = mask.to(device)\n",
    "                pred = model(img)\n",
    "                loss = loss_fn(pred,mask)\n",
    "                f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                epoch_running_f1 += f1_score\n",
    "                loss.backward()\n",
    "                if step_counter % 5 == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            # save the results: \n",
    "            train_losses[desc_key].append(epoch_running_loss/len(train_loader))\n",
    "            train_f1_scores[desc_key].append(epoch_running_f1/len(train_loader))    \n",
    "            \n",
    "            # save the model at specific intervals: \n",
    "            if epoch % 5 == 0:\n",
    "                model_path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/mlp_{desc_key}_epoch_{epoch}.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # TEST\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_running_loss = 0.0\n",
    "                epoch_running_f1 = 0.0\n",
    "                step_counter = 0\n",
    "                for img,mask in tqdm(test_loader):\n",
    "                    step_counter += 1\n",
    "                    img = img.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(img)\n",
    "                    loss = loss_fn(pred,mask)\n",
    "                    f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                    epoch_running_loss += loss.item()\n",
    "                    epoch_running_f1 += f1_score\n",
    "                    \n",
    "                # save the results: \n",
    "                test_losses[desc_key].append(epoch_running_loss/len(test_loader))\n",
    "                test_f1_scores[desc_key].append(epoch_running_f1/len(test_loader))    \n",
    "\n",
    "\n",
    "print(\"train losses:\",train_losses)\n",
    "print(\"train f1 scores:\",train_f1_scores)\n",
    "print(\"test losses:\",test_losses)\n",
    "print(\"test f1 scores:\",test_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each key in test_f1_scores; print the 0,5,10,15,20 epoch results\n",
    "\n",
    "max_thing = (0,0)\n",
    "max_val = 0.\n",
    "\n",
    "for key in test_f1_scores.keys():\n",
    "    print(f\"key: {key}\")\n",
    "    idxs = [0,5,10,15,20]\n",
    "    for idx in idxs:\n",
    "        print(f\"epoch: {idx}, loss: {test_losses[key][idx]}, f1: {test_f1_scores[key][idx]}\")\n",
    "        if test_f1_scores[key][idx] > max_val:\n",
    "            max_val = test_f1_scores[key][idx]\n",
    "            max_thing = (key,idx)\n",
    "            \n",
    "print(\"max thing:\",max_thing, \"max val:\",max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation reasons I want to submit two model predictions to the kaggle submission system:\n",
    "\n",
    "lr_0.0001_kaggle_epfl', 20: local F1: 94.89 -> Online 93.255\n",
    "lr_1e-05_kaggle, 15, local F1: 94.06 -> Online: 93.253\n",
    "\n",
    "Furthermore its also clear that the correlation with the epfl data is not so good. Since the spread in local and online F1 is much bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"custom_segment_anything/model_checkpoints/finetuned_kaggle/mlp_lr_0.0001_kaggle_epfl_epoch_20\"\n",
    "model_mlp_1 = load_sam_model(\"mlp\",device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "\n",
    "\n",
    "path2 = \"custom_segment_anything/model_checkpoints/finetuned_kaggle/mlp_lr_1e-05_kaggle_epoch_15\"\n",
    "model_mlp_2 = load_sam_model(\"mlp\",device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path2,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "\n",
    "#model_to_submission(model_mlp_1,submission_dataloader,submission_filename =  \"mlp_lr_0.0001_kaggle_epfl_epoch_20_submission.csv\")\n",
    "model_to_submission(model_mlp_2,submission_dataloader,submission_filename =  \"mlp_lr_1e-05_kaggle_epoch_15_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I want to finetune each model with the kaggle data\n",
    "\n",
    "The finetuning strategy will be\n",
    "\n",
    "batch_size = 15\n",
    "run for at most 15 epochs\n",
    "store for each decoder option, the best achieving f1 and score model seperately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### CLEAR ALL CUDA MEMORY\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "train_loader = kaggle_train_dataloader\n",
    "test_loader = kaggle_test_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader) // 10\n",
    "do_intermed_prints = False\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    "\n",
    "max_epochs = 15\n",
    "\n",
    "##################################\n",
    "# TRAINING LOOP\n",
    "################################\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    \n",
    "    # load the current model: \n",
    "    load_compiled = (decoder_option != \"mlp\")\n",
    "    model_description = f\"model_3_{decoder_option}_decoder_finetune_last_25_105_epoch_3\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=25,encoder_finetune_num_last_layers=105,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=load_compiled)\n",
    "    if not load_compiled:\n",
    "        model = torch.compile(model)\n",
    "    best_f1 = 0.\n",
    "    best_loss = 1000.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # training run: \n",
    "        model.train()\n",
    "        # reset the gradients: \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #########################################\n",
    "        # TRAINING LOOP\n",
    "        step_counter = 0\n",
    "        for image, mask in tqdm(train_loader):\n",
    "            step_counter += 1\n",
    "            #####################\n",
    "            # forward pass\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)    \n",
    "            pred = model(image)\n",
    "            # compute loss and f1 score: \n",
    "            loss = loss_fn(pred,mask)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            ###############\n",
    "            # backward pass\n",
    "            if step_counter % 5 == 0:\n",
    "                # compute the gradients\n",
    "                # update the model weights\n",
    "                optimizer.step()\n",
    "                # reset the gradients\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        ####### EVALUATION\n",
    "        \n",
    "        \n",
    "        # testing run: \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            l_acc = 0.0\n",
    "            score_acc = 0.0\n",
    "            for image,mask in tqdm(test_loader):\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "                pred = model(image)\n",
    "                # compute loss and f1 score: \n",
    "                score = mean_f1_score_from_logits(pred,mask)    \n",
    "                loss =  loss_fn(pred,mask)\n",
    "                # update running loss and f1 score\n",
    "                score_acc += score.item()\n",
    "                l_acc  += loss.item()\n",
    "                # store the loss and f1 score\n",
    "                \n",
    "            mean_f1 = score_acc/len(test_loader)\n",
    "            mean_loss = l_acc/len(test_loader)\n",
    "        \n",
    "        save_model_description_f1  = f\"decoder_{decoder_option}_bestf1.pth\"\n",
    "        save_model_description_loss = f\"decoder_{decoder_option}_bestloss.pth\"\n",
    "        file_path = \"custom_segment_anything/model_checkpoints/finetuned_kaggle/\"\n",
    "        \n",
    "        if mean_f1 > best_f1:\n",
    "            best_f1 = mean_f1\n",
    "            torch.save(model.state_dict(), file_path+save_model_description_f1)\n",
    "            print(\"Decoder option:\",decoder_option,\"new best f1:\",best_f1)\n",
    "        \n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(model.state_dict(), file_path+save_model_description_loss)\n",
    "            print(\"Decoder option:\",decoder_option,\"new best loss:\",best_loss)\n",
    "            \n",
    "        #########################################\n",
    "        \n",
    "    print(\"finished training for decoder option:\",decoder_option)\n",
    "    print(\"best f1:\",best_f1, \"best loss:\",best_loss)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finished training for decoder option: mlp\n",
    "best f1: 0.9406011423894337 best loss: 1.4403232016733714\n",
    "\n",
    "finished training for decoder option: conv\n",
    "best f1: 0.94107914183821 best loss: 1.443434430020196\n",
    "\n",
    "finished training for decoder option: spatial-small\n",
    "best f1: 0.9401421227625438 best loss: 1.4749739553247179\n",
    "\n",
    "finished training for decoder option: spatial-full\n",
    "best f1: 0.9409123829432896 best loss: 1.432130630527224\n",
    "\n",
    "finished training for decoder option: skip-connect\n",
    "best f1: 0.9407457964760917 best loss: 1.4466931287731444\n",
    "\n",
    "# Single Model Submission and Rounding Ensemble Submission of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "\n",
    "\n",
    "def model_to_submission(model,submission_dataloader,submission_filename =  \"dummy_submission.csv\"):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm(submission_dataloader):\n",
    "            image = image.to(device)\n",
    "            pred = model(image)\n",
    "            predictions.append(pred)\n",
    "    #print(len(predictions), predictions[0].shape)\n",
    "    # check the shape of the predictions\n",
    "    assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    for pred in predictions:\n",
    "        pred = pred.squeeze()\n",
    "        # pred is torch vector of shape (1024,1024)\n",
    "        # convert to image\n",
    "        pred = torch.round(torch.sigmoid(pred))\n",
    "        # compress to 400x400\n",
    "        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "        #print(pred.shape)\n",
    "        # pred is now torch vector of shape (1,1,400,400)\n",
    "        # convert to numpy\n",
    "        pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "        #print(pred.shape)\n",
    "        # pred is now numpy vector of shape (400,400)\n",
    "        # store as png to disk\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        pred = np.stack([pred,pred,pred],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", pred)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir)\n",
    "\n",
    "\n",
    "def ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"dummy_submission.csv\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    all_predictions = []\n",
    "    for model in model_list:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image in tqdm(submission_dataloader):\n",
    "                image = image.to(device)\n",
    "                pred = model(image)\n",
    "                predictions.append(pred)\n",
    "        model = model.to(\"cpu\")\n",
    "    # check the shape of the predictions\n",
    "        assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "        all_predictions.append(predictions)\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    num_models = len(model_list)\n",
    "    num_images = len(all_predictions[0])\n",
    "    assert num_images == 144\n",
    "    \n",
    "    for img_index in range(num_images):\n",
    "        \n",
    "        \n",
    "        ensemble_image = np.zeros((400,400))\n",
    "        \n",
    "        for model_idx in range(num_models):\n",
    "            pred = all_predictions[model_idx][img_index]\n",
    "            pred = pred.squeeze()\n",
    "            # pred is torch vector of shape (1024,1024)\n",
    "            # convert to image\n",
    "            pred = torch.round(torch.sigmoid(pred))\n",
    "            # compress to 400x400\n",
    "            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "            #print(pred.shape)\n",
    "            # pred is now torch vector of shape (1,1,400,400)\n",
    "            # convert to numpy\n",
    "            pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "            #print(pred.shape)\n",
    "            # pred is now numpy vector of shape (400,400)\n",
    "\n",
    "            # add to initial image\n",
    "            ensemble_image += pred\n",
    "        \n",
    "        # now the ensemble image is the sum of all predictions\n",
    "        # need to round and find most common prediction: it should be numbers in range of 0 to num_models\n",
    "        if rounding_policy == \"up\" or rounding_policy == \"down\":\n",
    "            ensemble_image = ensemble_image/num_models\n",
    "        \n",
    "            # round the image\n",
    "            if rounding_policy == \"up\":\n",
    "                ensemble_image = np.round(ensemble_image)\n",
    "            elif rounding_policy == \"down\":\n",
    "                ensemble_image = np.floor(ensemble_image)\n",
    "            else:\n",
    "                raise ValueError(\"invalid rounding policy\")\n",
    "        elif rounding_policy == \"min1\":\n",
    "            ensemble_image[ensemble_image > 1] = 1\n",
    "            #ensemble_image = np.round(ensemble_image)\n",
    "        else:\n",
    "            raise ValueError(\"invalid rounding policy\")\n",
    "        # store as png to disk\n",
    "        ensemble_image = (ensemble_image * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        ensemble_image = np.stack([ensemble_image,ensemble_image,ensemble_image],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", ensemble_image)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir, foreground_threshold=foreground_threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "def ensemble_model_loss_f1(model_list,test_dataloader,rounding_policy=\"up\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    for model in model_list:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    num_models = len(model_list)\n",
    "    f1_acc = 0.\n",
    "    \n",
    "    for image, mask in tqdm(test_dataloader):\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        ensemble_image = torch.zeros((1,1,1024,1024),device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model in model_list:\n",
    "                pred = model(image)\n",
    "                pred = torch.round(torch.sigmoid(pred))\n",
    "                ensemble_image = ensemble_image + pred\n",
    "        \n",
    "        # now the ensemble image is the sum of all predictions\n",
    "        # need to round and find most common prediction: it should be numbers in range of 0 to num_models\n",
    "        if rounding_policy == \"up\" or rounding_policy == \"down\":\n",
    "            ensemble_image = ensemble_image/num_models\n",
    "        \n",
    "            # round the image\n",
    "            if rounding_policy == \"up\":\n",
    "                ensemble_image = torch.round(ensemble_image)\n",
    "            elif rounding_policy == \"down\":\n",
    "                ensemble_image = torch.floor(ensemble_image)\n",
    "            else:\n",
    "                raise ValueError(\"invalid rounding policy\")\n",
    "        elif rounding_policy == \"min1\":\n",
    "            ensemble_image[ensemble_image > 1] = 1\n",
    "            #ensemble_image = np.round(ensemble_image)\n",
    "        else:\n",
    "            raise ValueError(\"invalid rounding policy\")\n",
    "        \n",
    "        # compute the f1 score here: \n",
    "        \n",
    "      \n",
    "        # compute the f1 score\n",
    "        f1_acc += mean_f1_score_from_classes(ensemble_image,mask)\n",
    "        \n",
    "    print(\"mean f1_score: \",f1_acc/len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing F1 - Score on Kaggle Test split of Majority Vote MLP BiSeSam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_loss_f1(model_list,kaggle_test_dataloader,rounding_policy=\"up\",foreground_threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Submission for the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_to_submission(model,submission_dataloader,submission_filename =  f\"decoder_{decoder_option}_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a majority vote ensemble of the models\n",
    "model_list = []\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_list.append(model)\n",
    "\n",
    "ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"majority_vote_ensemble_submission.csv\",foreground_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the majority vote mlp ensemble score on the current f1 test set split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Ensemble of N models\n",
    "\n",
    "The idea is to concatenate the logit predicitions of all models together, use the sigmoid on them and then feed this information into an mlp, and train this for 1 epoch. This then is the final mlp prediction.:\n",
    "\n",
    "Please note that the trained ensemble model is extremely specific to the models that were used to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now we test these models submission:\n",
    "\n",
    "# create a majority vote ensemble of the models\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"] # IMPORTANT IS THE CORRECT ORDER FOR ENSEMBLE TRAIN COMPUTATION/LOADING FROM MEMORY\n",
    "model_list = []\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_models = len(model_list)\n",
    "\n",
    "######## DEFINE MODEL ########\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import EnsembleMLP\n",
    "\n",
    "#gc\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# enable TF32\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "ensemble_model = EnsembleMLP(num_models)\n",
    "\n",
    "######### OPTIMIZER AND LOSS FUNCTION ########\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.0001)\n",
    "\n",
    "max_num_epochs=10\n",
    "####### DATA LOADER ##########\n",
    "train_loader = kaggle_train_dataloader\n",
    "eval_d_loader = kaggle_test_dataloader\n",
    "\n",
    "\n",
    "# bring all models to the device\n",
    "\n",
    "for model in model_list:\n",
    "    model = model.to(device)\n",
    "\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "##############################\n",
    "\n",
    "\n",
    "for epoch_counter in range(max_num_epochs):\n",
    "\n",
    "    # go over all models and feed the input image: \n",
    "\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    ###### TRAINING: #######\n",
    "    loss_acc = 0.0\n",
    "    f1_acc = 0.0\n",
    "    \n",
    "    step_counter = 0\n",
    "\n",
    "    for image, mask in tqdm(train_loader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        all_preds = torch.zeros((1,1,1024,1024,num_models)).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(num_models):\n",
    "                model = model_list[model_idx]\n",
    "\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "        \n",
    "        # compute the ensemble prediction\n",
    "        ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "        # compute the loss\n",
    "        #print(ensemble_pred.shape,mask.shape,ensemble_pred.min(),ensemble_pred.max(),mask.min(),mask.max())\n",
    "        loss = loss_fn(ensemble_pred,mask)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_acc += loss.item()\n",
    "        # compute the f1 score\n",
    "        f1_acc += mean_f1_score_from_logits(ensemble_pred,mask).item()\n",
    "\n",
    "    # print the loss and f1 score\n",
    "    print(f\"Epoch {epoch_counter} (train) Loss: {loss_acc/len(train_loader)}, F1-Score: {f1_acc/len(train_loader)}\")\n",
    "     \n",
    "        \n",
    "\n",
    "    torch.save(ensemble_model.state_dict(), f\"custom_segment_anything/model_checkpoints/ensemble_mlp/ensemble_model_{epoch_counter}.pth\")\n",
    "    \n",
    "    ###### TESTING: #######\n",
    "        \n",
    "    ensemble_model.eval()\n",
    "    loss_acc = 0.\n",
    "    f1_acc = 0.\n",
    "    for image, mask in tqdm(eval_d_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        all_preds = torch.zeros((1,1,1024,1024,num_models)).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(num_models):\n",
    "                model = model_list[model_idx]\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "        \n",
    "            # compute the ensemble prediction\n",
    "            ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "            # compute the loss\n",
    "\n",
    "            loss = loss_fn(ensemble_pred,mask)\n",
    "            loss_acc += loss.item()\n",
    "            # compute the f1 score\n",
    "            f1_acc += mean_f1_score_from_logits(ensemble_pred,mask).item()\n",
    "\n",
    "    # print the loss and f1 score\n",
    "    print(f\"Epoch {epoch_counter} (test) Loss: {loss_acc/len(eval_d_loader)}, F1-Score: {f1_acc/len(eval_d_loader)}\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now creating a submission with this trained ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epoche or 3 or 9? \n",
    "\n",
    "num_models = len(model_list)\n",
    "ensemble_model = EnsembleMLP(num_models)\n",
    "ensemble_model.load_state_dict(torch.load(\"custom_segment_anything/model_checkpoints/ensemble_mlp/ensemble_model_9.pth\"))\n",
    "\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "\n",
    "# do prediction using ensemble model: \n",
    "# use sigmoid and then round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "def trained_ensemble_models_to_submission(ensemble_model,model_list,submission_dataloader,submission_filename =  \"dummy_submission.csv\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert pass these logits to the ensemble model and then convert them to predictions using sigmoid and rounding\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # first compute all predictions with the model\n",
    "    counter = 144\n",
    "    \n",
    "    for image in tqdm(submission_dataloader):\n",
    "        image = image.to(device)    \n",
    "        all_preds = torch.zeros((1,1,1024,1024,len(model_list))).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(len(model_list)):\n",
    "                model = model_list[model_idx]\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "            # compute the ensemble prediction\n",
    "            ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "            # convert to numpy\n",
    "            #ensemble_pred = ensemble_pred\n",
    "            \n",
    "            # round the image\n",
    "            ensemble_image = torch.round(torch.sigmoid(ensemble_pred))\n",
    "            #print(ensemble_image.shape)\n",
    "            #print(\"HELLO\")\n",
    "            ensemble_image = F.interpolate(ensemble_image, size=(400,400), mode='nearest')\n",
    "            ensemble_image = ensemble_image.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "            #print(ensemble_image.shape)\n",
    "            # pred is now numpy vector of shape (400,400)\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "            # store as png to disk\n",
    "            ensemble_image = (ensemble_image * 255).astype(np.uint8)\n",
    "            # add 2 other color channels\n",
    "            ensemble_image = np.stack([ensemble_image,ensemble_image,ensemble_image],axis=2)\n",
    "            #print(pred.shape)\n",
    "            # save to disk\n",
    "            plt.imsave(path+\"mask_\"+str(counter)+\".png\", ensemble_image)\n",
    "            counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir, foreground_threshold= foreground_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ensemble_models_to_submission(ensemble_model,model_list,submission_dataloader,submission_filename =  \"mlp_ensemble_9.csv\", foreground_threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another ensemble with min 1 policy, its street, if at least one model says street;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models_to_submission(ensemble_model_list,submission_dataloader,rounding_policy=\"min1\",submission_filename =  \"ensemble_submission_top7_loss_min1_rounding.csv\",foreground_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 0.86517\n",
    "ours = 0.92984\n",
    "best = 0.93628\n",
    "grade = 4 + 2 * (ours - base)/(best - base)\n",
    "\n",
    "print(\"our grade:\",grade)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning SAM on Satellite Images for Street Segmentation\n",
    "\n",
    "Our approach is based on reusing the pretrained SAM Vision Transformer, more specifically using the image encoder and discarding the prompt encoder and mask decoder. \n",
    "\n",
    "Instead we will try out a combination of different Custom Encoder thats we use on the encoded images. \n",
    "\n",
    "We will try to use different approaches for the Decocer: \n",
    "\n",
    "1. Conv/Deconv based approach\n",
    "2. Fully connected MLP's\n",
    "3. Same as the mask decoder? \n",
    "4. Transformer, ViT? \n",
    "\n",
    "Another idea is to use adapter finetuning ?\n",
    "\n",
    "Furthermore things that can be modified are how many last layers of the SAM encoder are also fine tuned? \n",
    "Possibly just finetune them, after a while of training the newly initialized decoder. \n",
    "\n",
    "We will work with a lr schedule that reduces on plateau.\n",
    "\n",
    "\n",
    "---\n",
    "### Loss function: \n",
    "We use combination of the following: \n",
    "- DiceLoss\n",
    "- FocalLoss\n",
    "- BCEWithLogitsLoss\n",
    "---\n",
    "### Learning rate:\n",
    "- really small values\n",
    "- reduceLROnPlateau(optimizer, 'max', patience=reduce_patience, verbose=verbose, factor=reduce_factor)\n",
    "- cosine annealing\n",
    "\n",
    "\n",
    "TODO: \n",
    "\n",
    "- IMPLEMENT LR SCHEDULE AND STOPPING CRITERIA\n",
    "- CHECK OUT OTHER DECODERS SegFormer, MLP ... \n",
    "- CHECK OUT TO TRAIN MORE LAYERS OF TRANSFORMER\n",
    "\n",
    "# Novel Ideas:\n",
    "\n",
    "- LOOK AT VIT ARCHITECTURE\n",
    "- TRAIN FROM SCRATCH, VARIE SOMETHING WITH LOCAL/GLOBAL ATTENTION or try some COMBINATION RESIDUAL/SKIP CONNECTIONS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Jannek Ulm 16.5.2024\n",
    "# code was inspired by the following sources: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
    "\n",
    "from utils.image_loading import * \n",
    "from utils.torch_device import *\n",
    "from custom_datasets import Sat_Mask_Dataset, Sat_Only_Image_Dataset\n",
    "\n",
    "device = get_torch_device(allow_mps=True)\n",
    "print(\"using device:\",device)\n",
    "\n",
    "###########\n",
    "\n",
    "original_data = {}\n",
    "original_data[\"images\"] =load_training_images()\n",
    "original_data[\"masks\"] = load_groundtruth_images()\n",
    "\n",
    "city_names = [\"boston\",\"nyc\",\"zurich\"]\n",
    "custom_data = {\"images\":[],\"masks\":[]} # stores images and gt masks\n",
    "\n",
    "for name in city_names:\n",
    "    custom_data[\"images\"].extend(load_training_images(name))\n",
    "    custom_data[\"masks\"].extend(load_groundtruth_images(name))\n",
    "\n",
    "custom_data[\"images\"] = custom_data[\"images\"]#[0:200]\n",
    "custom_data[\"masks\"] = custom_data[\"masks\"]#[0:200]\n",
    "\n",
    "assert (len(custom_data[\"images\"]) == len(custom_data[\"masks\"]))\n",
    "\n",
    "\n",
    "print(\"the raw custom dataset contains\",len(custom_data[\"images\"]),\"images\")\n",
    "\n",
    "print(\"custom ds: (min,mean,max) street ratio\",get_street_ratio_mmm(custom_data[\"masks\"]))\n",
    "print(\"orig ds: (min,mean,max) street ratio\",get_street_ratio_mmm(original_data[\"masks\"]))\n",
    "\n",
    "# create a dataset\n",
    "custom_data_set = Sat_Mask_Dataset(custom_data[\"images\"], custom_data[\"masks\"],min_street_ratio=0.03,max_street_ratio=1.0)\n",
    "original_data_set = Sat_Mask_Dataset(original_data[\"images\"],original_data[\"masks\"])\n",
    "print(\"after cleanup, the dataset now contains\",len(custom_data_set),\"images\")\n",
    "\n",
    "\n",
    "# submission kaggle dataset\n",
    "\n",
    "kaggle_submission_images = load_test_images()\n",
    "submission_data_set = Sat_Only_Image_Dataset(kaggle_submission_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = custom_data_set\n",
    "print(dataset[0][0].shape, dataset[0][0].dtype, dataset[0][0].mean(), dataset[0][0].min(), dataset[0][0].max())\n",
    "print(dataset[0][1].shape, dataset[0][1].dtype, dataset[0][1].mean(), dataset[0][1].min(), dataset[0][1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the street ratio distribution of the dataset\n",
    "\n",
    "custom_ratios = get_street_ratio_distr(custom_data[\"masks\"])\n",
    "original_ratios = get_street_ratio_distr(original_data[\"masks\"])\n",
    "\n",
    "plt.hist(custom_ratios,40)\n",
    "plt.hist(original_ratios,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the dataset loading works as planned: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get some random training images\n",
    "idx = 0\n",
    "image, mask = custom_data_set[idx]\n",
    "\n",
    "img = np.array(image).astype(np.uint8)\n",
    "# swap first and third dimension\n",
    "img = np.swapaxes(img, 0, 2)\n",
    "mask = np.array(mask)\n",
    "mask = np.swapaxes(mask, 0, 2)\n",
    "\n",
    "# 1x2 plot: \n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img)\n",
    "\n",
    "# image + mask overlay\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Image + Mask\")\n",
    "print(\"img: \",img.shape,img.dtype,img.min(),img.max(),img.mean())\n",
    "mask2 = (mask * 255.).astype(np.uint8)\n",
    "print(\"mask2: \",mask2.shape,mask2.dtype,mask2.min(),mask2.max())\n",
    "cmb = img\n",
    "cmb[:,:,0] = mask2[:,:,0]  \n",
    "print(\"cmb: \",cmb.shape,cmb.dtype,cmb.min(),cmb.max(),cmb.mean())\n",
    "plt.imshow(cmb)\n",
    "\n",
    "# mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "print(img.min(), img.max(), mask.min(), mask.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "import torch.utils.data\n",
    "custom_train_dataset, custom_test_dataset = torch.utils.data.random_split(custom_data_set, [train_split, 1-train_split])\n",
    "original_train_dataset, original_test_dataset = torch.utils.data.random_split(original_data_set, [train_split, 1-train_split])\n",
    "\n",
    "\n",
    "print(len(custom_train_dataset), len(custom_test_dataset))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# CHECK IF THE FOR MULTIPLE NUM WORKERS THE CODE WORKS AS EXPECTED\n",
    "# SEEMS TO BE CAUSING ISSUES WITH LONG startup and shutdown times for each epoch\n",
    "\n",
    "# decided to drop last to make f1/loss score mean computation easier.\n",
    "\n",
    "#original_train_dataloader = DataLoader(original_train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "#original_test_dataloader = DataLoader(original_test_dataset, batch_size=batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "original_dataloader = DataLoader(original_data_set, batch_size=batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# dataloader for submission dataset: \n",
    "submission_dataloader = DataLoader(submission_data_set, batch_size=batch_size, shuffle=False, drop_last=False,num_workers=4,persistent_workers=True)\n",
    "\n",
    "#custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "#custom_test_dataloader = DataLoader(custom_test_dataset, batch_size=batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "custom_dataloader = DataLoader(custom_data_set, batch_size=batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ (CUSTOM SAM (stored in repo))\n",
    "from custom_segment_anything.segment_anything import sam_model_registry\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import *\n",
    "\n",
    "# load the model from checkpoints on disk:\n",
    "def load_sam_decoder_model_from_checkpoint(checkpoint_idx:int, device,decoder,encoder_finetune_num_last_layers=6):\n",
    "    sam_checkpoint_path = \"custom_segment_anything/model_checkpoints/\"\n",
    "    # base, large, huge checkpoints. \n",
    "    checkpoint_names = [\"vit_b\",\"vit_l\",\"vit_h\"]\n",
    "    checkpoints = [\"sam_vit_b_01ec64.pth\",\"sam_vit_l_0b3195.pth\",\"sam_vit_h_4b8939.pth\"]\n",
    "    model_paths = [sam_checkpoint_path+checkpoint_name for checkpoint_name in checkpoints]\n",
    "    sam = sam_model_registry[checkpoint_names[checkpoint_idx]](checkpoint=model_paths[checkpoint_idx])\n",
    "    sam.to(device)\n",
    "    model = SAM_Encoder_Custom_Decoder(sam.preprocess, sam.image_encoder,decoder=decoder,encoder_finetune_num_last_layers=encoder_finetune_num_last_layers)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the finetnued model state, if already started training. \n",
    "def load_finetuned_model(name,device,decoder,encoder_finetune_num_last_layers=6):\n",
    "    model = load_sam_decoder_model_from_checkpoint(0,device,decoder,encoder_finetune_num_last_layers)\n",
    "    finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "    model.load_state_dict(torch.load(finetune_path+name,map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "def load_sam_model(decoder_option, device, encoder_finetune_num_last_layers, sam_checkpoint_idx=0,finetuned_model_name=\"model.pth\",sam_checkpoint_or_finetuned=\"sam\"):\n",
    "    #   \n",
    "    #   decoder_options: [\"conv\", \"mlp\" , \"segformer_mlp\"]\n",
    "    #   encoder_finetune_num_last_layers tells how many layers of sam encoder are finetuned, all decoder layers are tuned. \n",
    "    #   sam_checkpoint_or_finetuned checks if the model is loaded from a sam checkpoint or a finetuned model with the same architecture.\n",
    "    #\n",
    "\n",
    "\n",
    "    # first construct the model from sam_checkpoint:\n",
    "\n",
    "    if decoder_option == \"conv\":\n",
    "        decoder = Conv_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(sam_checkpoint_idx,device,decoder.decoder,encoder_finetune_num_last_layers)\n",
    "   \n",
    "    elif decoder_option == \"mlp\":\n",
    "        decoder = MLP_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(sam_checkpoint_idx,device,decoder,encoder_finetune_num_last_layers)\n",
    "\n",
    "    elif decoder_option == \"segformer_mlp\":\n",
    "        raise NotImplementedError(\"segformer_mlp decoder not implemented yet\")\n",
    "    else:\n",
    "        raise ValueError(\"invalid decoder option\")\n",
    "    \n",
    "    # if should load from fine-tuned model, load the model from the finetuned path.\n",
    "    if sam_checkpoint_or_finetuned == \"finetuned\":\n",
    "        finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "        model.load_state_dict(torch.load(finetune_path+finetuned_model_name+\".pth\",map_location=torch.device('cpu')))\n",
    "    elif sam_checkpoint_or_finetuned == \"sam\":\n",
    "        pass\n",
    "        # already initialized model from sam_checkpoint\n",
    "    else: \n",
    "        raise ValueError(\"invalid sam_checkpoint_or_finetuned option\")\n",
    "    \n",
    "    \n",
    "    # Unfreeze last layers of the encoder\n",
    "    for layer_number, param in enumerate(model.sam_encoder.parameters()):\n",
    "        if layer_number > 176 - encoder_finetune_num_last_layers:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze neck of the encoder\n",
    "    model.sam_encoder.neck.requires_grad = True\n",
    "    model.requires_grad = True\n",
    "    print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = \"model_best_so_far.pth\"\n",
    "model = load_sam_model(\"mlp\",device,encoder_finetune_num_last_layers=0,sam_checkpoint_idx=0,finetuned_model_name=finetuned_model_name,sam_checkpoint_or_finetuned=\"sam\")\n",
    "\n",
    "\n",
    "print(\"Model loaded\")\n",
    "# how many trainable parameters does the model have?\n",
    "print(\"Trainable parameters\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# how many total parameters does the model have?\n",
    "print(\"Total parameters\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# percentage of trainable parameters\n",
    "print(\"Percentage of trainable parameters: \")\n",
    "print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "def mean_f1_score_from_logits(pred,mask):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # compute the mean for all the images\n",
    "    # computes the mean over the 0-th axis\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_f1_score_from_classes(mask,pred_classes)\n",
    "\n",
    "\n",
    "def mean_f1_score_from_classes(preds,masks):\n",
    "    \n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # this computes the f1 over the whole batch, for each image in the batch alone:\n",
    "    \n",
    "    # first reshape the tensors\n",
    "    b_size = masks.shape[0]\n",
    "    f1_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i]\n",
    "        pred = preds[i]\n",
    "        # reshape and compute f1\n",
    "        f1_acc = f1_acc + multiclass_f1_score(pred.reshape((size)),mask.reshape((size)))\n",
    "        \n",
    "    mean_f1 = f1_acc/b_size\n",
    "    return mean_f1\n",
    "\n",
    "def dice_loss(logits,masks, smooth=1e-6):\n",
    "    \n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs_flat = probs.reshape(-1)\n",
    "    masks_flat = probs.reshape(-1)\n",
    "    \n",
    "    intersection = (probs_flat * masks_flat).sum()\n",
    "    union = probs_flat.sum() + masks_flat.sum()\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_coeff\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([1./0.13]).to(device)  # Example weights: adjust based on your dataset\n",
    "bce_loss = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "def focal_loss(logits, masks, alpha=0.15, gamma=2.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    loss = sigmoid_focal_loss(probs, masks, alpha=alpha, gamma=gamma, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def combined_loss_1(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return 2 * dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_2(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_3(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return dice_loss(logits, masks, smooth=smooth) + 2 * bce_loss(logits_sq, mask_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now add the logic \n",
    "\n",
    "go over all possible decoder models\n",
    "\n",
    "train for a few epochs with only few encoder layers unlocked until the last epoch didnt improve the original loss\n",
    "increase the number of layers finetuned, \n",
    "if the whole epoch didnt improve, then stop the training in general\n",
    "\n",
    "always store the model, start each new layer round with the best of the last or second last stored model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "train_loader = custom_dataloader\n",
    "test_loader = original_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "max_num_epochs = 3\n",
    "\n",
    "\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader) // 1\n",
    "do_intermed_prints = False\n",
    "#########\n",
    "\n",
    "decoder_options = [\"conv\", \"mlp\"]\n",
    "num_layers_to_finetune = [25,65,85,105]\n",
    "learning_rates = [0.001,0.0001, 0.00001, 0.00001]\n",
    "loss_functions = [combined_loss_1,combined_loss_2,combined_loss_3]\n",
    "\n",
    "for loss_fn_idx in range(len(loss_functions)):\n",
    "    loss_fn = loss_functions[loss_fn_idx]\n",
    "    for decoder_option in decoder_options:\n",
    "\n",
    "        # for the first step load a model from the sam checkpoint.\n",
    "        # in the second iteration load the model from the finetuned model checkpoint.\n",
    "        # train for at most 5 epochs or go on the next layer option if the last epoch didnt improve the loss. original loss,\n",
    "        best_last_epoch = 0\n",
    "        scheduler_step_count = 0\n",
    "        print(\"now running for decoder option: \",decoder_option)\n",
    "\n",
    "        for idx_layer_option in range(len(num_layers_to_finetune)):\n",
    "            layer_option = num_layers_to_finetune[idx_layer_option]\n",
    "            learning_rate = learning_rates[idx_layer_option]\n",
    "            print(\"now running for layer option: \",layer_option,\"decoder option: \",decoder_option)\n",
    "            best_epoch_loss = 101.\n",
    "            last_epoch_loss = 100.\n",
    "            \n",
    "            # load the initial model from the sam checkpoint\n",
    "            if idx_layer_option == 0:\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=layer_option,sam_checkpoint_idx=0,finetuned_model_name=None,sam_checkpoint_or_finetuned=\"sam\")\n",
    "            else: \n",
    "                model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{num_layers_to_finetune[idx_layer_option-1]}_epoch_{best_last_epoch}\"\n",
    "                print(\"loading model:\",model_description)\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=layer_option,sam_checkpoint_idx=0,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "            # newly initializing the optimizer and scheduler since model was loaded new\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # store all losses and f1 scores for the training and testing runs\n",
    "\n",
    "            losses = {\"train\":[],\"test\":[]}\n",
    "            f1_scores = {\"train\":[],\"test\":[]}\n",
    "\n",
    "            grad_update_step_counter = 0\n",
    "\n",
    "            epoch_counter = 0\n",
    "\n",
    "            while epoch_counter < max_num_epochs and last_epoch_loss <= best_epoch_loss:\n",
    "                print(\"Starting Epoch: \",epoch_counter)\n",
    "                # training run: \n",
    "                model.train()\n",
    "                # store running losses for the epoch and the 10% print interval\n",
    "                epoch_running_loss = 0.0\n",
    "                epoch_running_f1 = 0.0\n",
    "\n",
    "                short_running_loss = 0.0\n",
    "                short_running_f1 = 0.0\n",
    "\n",
    "                step_counter = 1\n",
    "                # reset the gradients: \n",
    "                \n",
    "                for image, mask in tqdm(train_loader):\n",
    "                    # forward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)    \n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    loss = loss_fn(pred,mask)\n",
    "                    # gradient accumulation\n",
    "                    loss.backward()\n",
    "                    # store the gradient information\n",
    "                    # update the model weights\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                    epoch_running_loss += loss.item()\n",
    "                    epoch_running_f1 += f1_score\n",
    "                    short_running_loss += loss.item()\n",
    "                    short_running_f1 += f1_score\n",
    "\n",
    "                    losses[\"train\"].append(loss.item())\n",
    "                    f1_scores[\"train\"].append(f1_score)\n",
    "\n",
    "                    if do_intermed_prints and step_counter % print_interval == 0:\n",
    "                        print(\"step: \",step_counter//print_interval)\n",
    "                        # print out the current losses:\n",
    "                        print(f\"Epoch: {epoch_counter}, step: {step_counter//print_interval}, (train) Loss: {short_running_loss/print_interval}, F1: {short_running_f1/print_interval}\")\n",
    "                        # and reset the short running losses\n",
    "                        short_running_loss = 0.0\n",
    "                        short_running_f1 = 0.0\n",
    "\n",
    "                    # increment the step counter: \n",
    "                    step_counter += 1\n",
    "                \n",
    "                \n",
    "                print(f\"Epoch: {epoch_counter}, (train) Loss: {epoch_running_loss/len(train_loader)}, F1: {epoch_running_f1/len(train_loader)}\")\n",
    "\n",
    "                # save the model in every epoch\n",
    "                model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option}_epoch_{epoch_counter}\"\n",
    "                print(\"saving model:\",model_description)\n",
    "                torch.save(model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+model_description+\".pth\")\n",
    "                \n",
    "                # testing run: \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    l_acc = 0.0\n",
    "                    score_acc = 0.0\n",
    "                    for image,mask in tqdm(test_loader):\n",
    "                        image = image.to(device)\n",
    "                        mask = mask.to(device)\n",
    "                        pred = model(image)\n",
    "                        # compute loss and f1 score: \n",
    "                        score = mean_f1_score_from_logits(pred,mask)    \n",
    "                        loss =  loss_fn(pred,mask)\n",
    "                        # update running loss and f1 score\n",
    "                        score_acc += score.item()\n",
    "                        l_acc  += loss.item()\n",
    "                        # store the loss and f1 score\n",
    "                        losses[\"test\"].append(loss.item())\n",
    "                        f1_scores[\"test\"].append(score.item())\n",
    "\n",
    "                    print(f\"Epoch: {epoch_counter}, (test) Loss: {l_acc/len(test_loader)}, F1-Score: {score_acc/len(test_loader)}\")    \n",
    "\n",
    "                    last_epoch_loss = l_acc/len(test_loader)\n",
    "                    if last_epoch_loss < best_epoch_loss:\n",
    "                        best_epoch_loss = last_epoch_loss\n",
    "                        best_last_epoch = epoch_counter\n",
    "                        print(\"new best epoch loss: \",best_epoch_loss)\n",
    "                    else: \n",
    "                        print(\"no improvement in loss, stopping training for this layer option\")\n",
    "                epoch_counter += 1\n",
    "\n",
    "            # save the model after the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to manually finetune each model with another epoch with lower learning rate and more layers to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mlp_name = \"model_mlp_decoder_finetune_last_95_epoch_0\"\n",
    "conv_name= \"model_conv_decoder_finetune_last_95_epoch_0\"\n",
    "\n",
    "\n",
    "# first show the current loss and f1 on original dataset: \n",
    "\n",
    "mlp_model = load_sam_model(\"mlp\",device,encoder_finetune_num_last_layers=95,sam_checkpoint_idx=0,finetuned_model_name=mlp_name,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "conv_model = load_sam_model(\"conv\",device,encoder_finetune_num_last_layers=95,sam_checkpoint_idx=0,finetuned_model_name=conv_name,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "print(device)\n",
    "print(\"before training:\")\n",
    "mlp_model = mlp_model.to(device)\n",
    "conv_model = conv_model.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "eval_d_loader = original_dataloader\n",
    "mlp_model.eval()\n",
    "conv_model.eval()\n",
    "with torch.no_grad():\n",
    "    l_acc_mlp = 0.0\n",
    "    score_acc_mlp = 0.0\n",
    "    l_acc_conv = 0.0\n",
    "    score_acc_conv = 0.0\n",
    "    \n",
    "    for image,mask in tqdm(eval_d_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        \n",
    "        pred_conv = conv_model(image)\n",
    "        pred_mlp = mlp_model(image)\n",
    "        \n",
    "        # compute loss and f1 score: \n",
    "        score_acc_conv += mean_f1_score_from_logits(pred_conv,mask).item()\n",
    "        l_acc_conv  += combined_loss_1(pred_conv,mask).item() \n",
    "        \n",
    "        l_acc_mlp += combined_loss_1(pred_mlp,mask).item() \n",
    "        score_acc_mlp +=  mean_f1_score_from_logits(pred_mlp,mask).item()\n",
    "        \n",
    "    print(f\"MLP (test) Loss: {l_acc_mlp/len(eval_d_loader)}, F1-Score: {score_acc_mlp/len(eval_d_loader)}\") \n",
    "    print(f\"CONV (test) Loss: {l_acc_conv/len(eval_d_loader)}, F1-Score: {score_acc_conv/len(eval_d_loader)}\") \n",
    "    \n",
    "    \n",
    "##################################################\n",
    "print(\"start training: ############################################\")\n",
    "# now train for one epoch with lower learning rate:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_model.train()\n",
    "mlp_model.train()\n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(mlp_model.parameters(),   lr=0.00001)\n",
    "optimizer_conv = torch.optim.Adam(conv_model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "# store running losses for the epoch and the 10% print interval\n",
    "l_acc_mlp = 0.0\n",
    "score_acc_mlp = 0.0\n",
    "l_acc_conv = 0.0\n",
    "score_acc_conv = 0.0\n",
    "\n",
    "# reset the gradients: \n",
    "\n",
    "for image, mask in tqdm(train_loader):\n",
    "    # forward pass\n",
    "    optimizer_conv.zero_grad()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    \n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)   \n",
    "    \n",
    "     \n",
    "    pred_conv = conv_model(image)\n",
    "    pred_mlp = mlp_model(image)\n",
    "    \n",
    "    # compute loss and f1 score: \n",
    "    score_acc_conv += mean_f1_score_from_logits(pred_conv,mask).item()\n",
    "    conv_loss = combined_loss_1(pred_conv,mask)\n",
    "    l_acc_conv += conv_loss.item() \n",
    "    \n",
    "    mlp_loss = combined_loss_1(pred_mlp,mask)\n",
    "    l_acc_mlp += mlp_loss.item() \n",
    "    score_acc_mlp +=  mean_f1_score_from_logits(pred_mlp,mask).item()\n",
    "    \n",
    "    conv_loss.backward()\n",
    "    mlp_loss.backward()\n",
    "    \n",
    "    optimizer_mlp.step()\n",
    "    optimizer_conv.step()\n",
    "    \n",
    "print(f\"MLP (test) Loss: {l_acc_mlp/len(train_loader)}, F1-Score: {score_acc_mlp/len(train_loader)}\") \n",
    "print(f\"CONV (test) Loss: {l_acc_conv/len(train_loader)}, F1-Score: {score_acc_conv/len(train_loader)}\") \n",
    "\n",
    "# save the models: \n",
    "\n",
    "print(\"done training ############################\")\n",
    "\n",
    "torch.save(conv_model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+\"model_conv_decoder_finetune_last_95_epoch_1\"+\".pth\")\n",
    "torch.save(mlp_model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+\"model_mlp_decoder_finetune_last_95_epoch_1\"+\".pth\")\n",
    "\n",
    "\n",
    "####################### EVALUATION AFTER TRAINING\n",
    "eval_d_loader = original_dataloader\n",
    "mlp_model.eval()\n",
    "conv_model.eval()\n",
    "with torch.no_grad():\n",
    "    l_acc_mlp = 0.0\n",
    "    score_acc_mlp = 0.0\n",
    "    l_acc_conv = 0.0\n",
    "    score_acc_conv = 0.0\n",
    "    \n",
    "    for image,mask in tqdm(eval_d_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        \n",
    "        pred_conv = conv_model(image)\n",
    "        pred_mlp = mlp_model(image)\n",
    "        \n",
    "        # compute loss and f1 score: \n",
    "        score_acc_conv += mean_f1_score_from_logits(pred_conv,mask).item()\n",
    "        l_acc_conv  += combined_loss_1(pred_conv,mask).item() \n",
    "        \n",
    "        l_acc_mlp += combined_loss_1(pred_mlp,mask).item() \n",
    "        score_acc_mlp +=  mean_f1_score_from_logits(pred_mlp,mask).item()\n",
    "        \n",
    "    print(f\"MLP (test) Loss: {l_acc_mlp/len(eval_d_loader)}, F1-Score: {score_acc_mlp/len(eval_d_loader)}\") \n",
    "    print(f\"CONV (test) Loss: {l_acc_conv/len(eval_d_loader)}, F1-Score: {score_acc_conv/len(eval_d_loader)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we test these models submission:\n",
    "\n",
    "#model_to_submission(conv_model,submission_dataloader,submission_filename =  \"conv_85_2_submission.csv\")\n",
    "#model_to_submission(mlp_model,submission_dataloader,submission_filename =  \"mlp_95_1_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mlp_name = \"model_mlp_decoder_finetune_last_95_epoch_0\"\n",
    "conv_name= \"model_1_conv_decoder_finetune_last_85_epoch_0\"\n",
    "\n",
    "\n",
    "# first show the current loss and f1 on original dataset: \n",
    "\n",
    "conv_model = load_sam_model(\"conv\",device,encoder_finetune_num_last_layers=95,sam_checkpoint_idx=0,finetuned_model_name=conv_name,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "model_to_submission(conv_model,submission_dataloader,submission_filename =  \"conv_loss_1_85_0_submission.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and F1 of current model on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = f1_loss\n",
    "import torch.nn as nn\n",
    "\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "eval_d_loader = original_dataloader\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    l_acc = 0.0\n",
    "    score_acc = 0.0\n",
    "    \n",
    "    for image,mask in tqdm(eval_d_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        pred = model(image)\n",
    "        # compute loss and f1 score: \n",
    "        score = mean_f1_score_from_logits(pred,mask)\n",
    "        loss = combined_loss_1(pred,mask)\n",
    "        \n",
    "        score_acc += score.item()\n",
    "        l_acc  += loss.item() \n",
    "    print(f\"(test) Loss: {l_acc/len(eval_d_loader)}, F1-Score: {score_acc/len(eval_d_loader)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN CODE FROM KAGGLE: \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"SAM model + Custom Decoder set to EVAL mode\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sample = original_data_set[10]\n",
    "inpt_0, gt_0 = sample[0], sample[1]\n",
    "with torch.no_grad():\n",
    "    decoder_opt = model(inpt_0.to(device).unsqueeze(0))\n",
    "print(decoder_opt.shape)\n",
    "\n",
    "decoder_opt_np = ((decoder_opt > 0.5)*1).to(\"cpu\").numpy()[0].transpose(1,2,0)\n",
    "gt_0_np = gt_0.to(\"cpu\").numpy().transpose(1,2,0)\n",
    "print(\"Np arr shape: \", decoder_opt_np.shape, gt_0_np.shape)\n",
    "\n",
    "temp_img_np = inpt_0.to(\"cpu\").numpy()\n",
    "temp_img_np = np.transpose(temp_img_np, [1,2,0])\n",
    "temp_img_np = temp_img_np.astype(np.uint8)\n",
    "\n",
    "# Create a figure with one row and two columns of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "# Display img1 on the first subplot\n",
    "axs[0].imshow(gt_0_np)\n",
    "# Hide the axes of the first subplot\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Display img2 on the second subplot\n",
    "axs[1].imshow(decoder_opt_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Display img3 on the second subplot\n",
    "axs[2].imshow(temp_img_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[2].axis('off')\n",
    "\n",
    "# Adjust the spacing between the subplots\n",
    "fig.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_loss\",losses[\"train\"])\n",
    "print(\"test loss\", losses[\"test\"])\n",
    "print(\"train_f1\",f1_scores[\"train\"])\n",
    "print(\"test_f1\",f1_scores[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model_best_so_far.pth\")\n",
    " \n",
    "decoder = Conv_Decoder()\n",
    "name = \"model_finetune_30_dice_epoch3.pth\"\n",
    "model = load_finetuned_model(name,device,decoder,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Submission: \n",
    "\n",
    "First run the model over the submission dataloader, generate all the images. Then transform the 1024x1024 images back to the required format and store them on disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "\n",
    "\n",
    "def model_to_submission(model,submission_dataloader,submission_filename =  \"dummy_submission.csv\"):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm(submission_dataloader):\n",
    "            image = image.to(device)\n",
    "            pred = model(image)\n",
    "            predictions.append(pred)\n",
    "    #print(len(predictions), predictions[0].shape)\n",
    "    # check the shape of the predictions\n",
    "    assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    for pred in predictions:\n",
    "        pred = pred.squeeze()\n",
    "        # pred is torch vector of shape (1024,1024)\n",
    "        # convert to image\n",
    "        pred = torch.round(torch.sigmoid(pred))\n",
    "        # compress to 400x400\n",
    "        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "        #print(pred.shape)\n",
    "        # pred is now torch vector of shape (1,1,400,400)\n",
    "        # convert to numpy\n",
    "        pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "        #print(pred.shape)\n",
    "        # pred is now numpy vector of shape (400,400)\n",
    "        # store as png to disk\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        pred = np.stack([pred,pred,pred],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", pred)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_submission(model,submission_dataloader,submission_filename =  \"test_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show images from submission dataset with their prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN CODE FROM KAGGLE: \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"SAM model + Custom Decoder set to EVAL mode\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    " \n",
    "inpt_0 = submission_data_set[2]\n",
    "with torch.no_grad():\n",
    "    decoder_opt = model(inpt_0.to(device).unsqueeze(0))\n",
    "print(decoder_opt.shape)\n",
    "\n",
    "decoder_opt_np = ((decoder_opt > 0.5)*1).to(\"cpu\").numpy()[0].transpose(1,2,0)\n",
    "print(\"Np arr shape: \", decoder_opt_np.shape)\n",
    "\n",
    "temp_img_np = inpt_0.to(\"cpu\").numpy()\n",
    "temp_img_np = np.transpose(temp_img_np, [1,2,0])\n",
    "temp_img_np = temp_img_np.astype(np.uint8)\n",
    "\n",
    "# Create a figure with one row and two columns of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display img2 on the second subplot\n",
    "axs[0].imshow(decoder_opt_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Display img3 on the second subplot\n",
    "axs[1].imshow(temp_img_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Adjust the spacing between the subplots\n",
    "fig.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning SAM on Satellite Images for Street Segmentation\n",
    "\n",
    "Our approach is based on reusing the pretrained SAM Vision Transformer, more specifically using the image encoder and discarding the prompt encoder and mask decoder. \n",
    "\n",
    "Instead we will try out a combination of different Custom Encoder thats we use on the encoded images. \n",
    "\n",
    "We will try to use different approaches for the Decocer: \n",
    "\n",
    "1. Conv/Deconv based approach\n",
    "2. Fully connected MLP's\n",
    "3. Same as the mask decoder? \n",
    "4. Transformer, ViT? \n",
    "\n",
    "\n",
    "Furthermore things that can be modified are how many last layers of the SAM encoder are also fine tuned? \n",
    "Possibly just finetune them, after a while of training the newly initialized decoder. \n",
    "\n",
    "We will work with a lr schedule that reduces on plateau.\n",
    "\n",
    "Loss function? \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Loss function: \n",
    "- F1-Loss: https://pytorch.org/torcheval/stable/generated/torcheval.metrics.functional.multiclass_f1_score.html\n",
    "---\n",
    "### Learning rate:\n",
    "- really small values\n",
    "- reduceLROnPlateau(optimizer, 'max', patience=reduce_patience, verbose=verbose, factor=reduce_factor)\n",
    "- cosine annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <CDAC6E34-8608-3E70-8B2F-32BCD38E90FB> /Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# written by Jannek Ulm 16.5.2024\n",
    "# code was inspired by the following sources: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
    "\n",
    "from utils.image_loading import * \n",
    "from utils.torch_device import *\n",
    "from custom_datasets import Sat_Mask_Dataset\n",
    "\n",
    "device = get_torch_device(allow_mps=True)\n",
    "print(device)\n",
    "\n",
    "###########\n",
    "\n",
    "original_data = {}\n",
    "original_data[\"images\"] =load_training_images()\n",
    "original_data[\"masks\"] = load_groundtruth_images()\n",
    "\n",
    "city_names = [\"boston\",\"nyc\",\"zurich\"]\n",
    "custom_data = {\"images\":[],\"masks\":[]} # stores images and gt masks\n",
    "\n",
    "for name in city_names:\n",
    "    custom_data[\"images\"].extend(load_training_images(name))\n",
    "    custom_data[\"masks\"].extend(load_groundtruth_images(name))\n",
    "\n",
    "assert (len(custom_data[\"images\"]) == len(custom_data[\"masks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9208\n",
      "custom ds: (0.0, 0.1329543182558652, 0.72180625)\n",
      "orig ds: (0.008968750000000001, 0.17797695312500006, 0.40426875)\n"
     ]
    }
   ],
   "source": [
    "print(len(custom_data[\"images\"]))\n",
    "\n",
    "print(\"custom ds:\",get_street_ratio_mmm(custom_data[\"masks\"]))\n",
    "print(\"orig ds:\",get_street_ratio_mmm(original_data[\"masks\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the street ratio distribution of the dataset\n",
    "\n",
    "custom_ratios = get_street_ratio_distr(custom_data[\"masks\"])\n",
    "original_ratios = get_street_ratio_distr(original_data[\"masks\"])\n",
    "\n",
    "plt.hist(custom_ratios,40)\n",
    "plt.hist(original_ratios,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialzed dataset, checked for min,max street ratio. Discarded %: 0.09165942658557776  num discarded: 844\n",
      "Initialzed dataset, checked for min,max street ratio. Discarded %: 0.0  num discarded: 0\n",
      "8364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a dataset\n",
    "custom_data_set = Sat_Mask_Dataset(custom_data[\"images\"], custom_data[\"masks\"],min_street_ratio=0.01)\n",
    "original_data_set = Sat_Mask_Dataset(original_data[\"images\"],original_data[\"masks\"])\n",
    "print(len(custom_data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the dataset loading works as planned: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get some random training images\n",
    "idx = 0\n",
    "image, mask = custom_data_set[idx]\n",
    "\n",
    "img = np.array(image)\n",
    "# swap first and third dimension\n",
    "img = np.swapaxes(img, 0, 2)\n",
    "mask = np.array(mask)\n",
    "mask = np.swapaxes(mask, 0, 2)\n",
    "\n",
    "# 1x2 plot: \n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img)\n",
    "\n",
    "# image + mask overlay\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Image + Mask\")\n",
    "plt.imshow(img + mask)\n",
    "\n",
    "# mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(img.min(), img.max(), mask.min(), mask.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "import torch.utils.data\n",
    "custom_train_dataset, custom_test_dataset = torch.utils.data.random_split(custom_data_set, [train_split, 1-train_split])\n",
    "original_train_dataset, original_test_dataset = torch.utils.data.random_split(original_data_set, [train_split, 1-train_split])\n",
    "\n",
    "\n",
    "print(len(custom_train_dataset), len(custom_test_dataset))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# CHECK IF THE FOR MULTIPLE NUM WORKERS THE CODE WORKS AS EXPECTED\n",
    "# SEEMS TO BE CAUSING ISSUES WITH LONG startup and shutdown times for each epoch\n",
    "\n",
    "train_dataloader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, drop_last=False,num_workers=0)\n",
    "test_dataloader = DataLoader(custom_test_dataset, batch_size=batch_size, shuffle=True, drop_last=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image,mask in train_dataloader:\n",
    "    print(image.shape, mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ (CUSTOM SAM (stored in repo))\n",
    "from custom_segment_anything.segment_anything import sam_model_registry\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import *\n",
    "\n",
    "\n",
    "sam_checkpoint_path = \"custom_segment_anything/model_checkpoints/\"\n",
    "\n",
    "# base, large, huge checkpoints. \n",
    "checkpoints = [\"sam_vit_b_01ec64.pth\",\"sam_vit_l_0b3195.pth\",\"sam_vit_h_4b8939.pth\"]\n",
    "checkpoint_names = [\"vit_b\",\"vit_l\",\"vit_h\"]\n",
    "\n",
    "# 0,1,2 for base, large, huge checkpoints: \n",
    "checkpoint_idx = 0#,1,2\n",
    "\n",
    "model_paths = [sam_checkpoint_path+checkpoint_name for checkpoint_name in checkpoints]\n",
    "\n",
    "\n",
    "sam = sam_model_registry[checkpoint_names[checkpoint_idx]](checkpoint=model_paths[checkpoint_idx])\n",
    "sam.to(device)\n",
    "conv_decoder = Conv_Decoder()\n",
    "\n",
    "model = SAM_Encoder_Custom_Decoder(sam.preprocess, sam.image_encoder,decoder=conv_decoder.decoder,encoder_finetune_num_last_layers=6)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:::\n",
    "\n",
    "Note that right now the model is not using a sigmoid function in the last layer. \n",
    "\n",
    "implement the right loss function and test out different versions, add saving of the model version after training \n",
    "\n",
    "add evaluation of test dataloader to see prediction result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the finetnued model state, if already started training. \n",
    "\n",
    "#finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "#name = \"model_epoch3.pth\"\n",
    "# \n",
    "#model.load_state_dict(torch.load(finetune_path+name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Model loaded\")\n",
    "# how many trainable parameters does the model have?\n",
    "print(\"Trainable parameters\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# how many total parameters does the model have?\n",
    "print(\"Total parameters\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# percentage of trainable parameters\n",
    "print(\"Percentage of trainable parameters: \")\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "\n",
    "for epoch in range(3,num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for image, mask in train_dataloader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # first steps of the model with no gradients\n",
    "        \n",
    "        x =model.sam_preprocess(image)\n",
    "        x = model.sam_encoder(x)\n",
    "        pred = model.decoder(x)\n",
    "        \n",
    "        #pred = model(image)\n",
    "\n",
    "        # pred has shape (batch_size, 1, 1024, 1024)\n",
    "        pred = pred[:,0,:,:] # [batch_size, 1024, 1024]\n",
    "        # mask has shape (batch_size, 3, 1024, 1024)\n",
    "        mask = mask[:,0,:,:] # [batch_size, 1024, 1024]\n",
    "\n",
    "        # stretch the pred to one dimension\n",
    "        loss = bce_loss(pred, mask)\n",
    "\n",
    "        #loss = multiclass_f1_score(pred, mask,num_classes=2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(loss.item(),\"step:\",i,\"of\",len(train_dataloader))\n",
    "        i += 1\n",
    "    print(f\"Epoch: {epoch}, Loss: {running_loss/len(train_dataset)}\")\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        # save the model\n",
    "        torch.save(model.state_dict(), f\"model_epoch{epoch}.pth\")\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), \"model_tr1.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

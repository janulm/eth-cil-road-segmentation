{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning SAM on Satellite Images for Street Segmentation\n",
    "\n",
    "Our approach is based on reusing the pretrained SAM Vision Transformer, more specifically using the image encoder and discarding the prompt encoder and mask decoder. \n",
    "\n",
    "Instead we will try out a combination of different Custom Encoder thats we use on the encoded images. \n",
    "\n",
    "We will try to use different approaches for the Decocer: \n",
    "\n",
    "1. Conv/Deconv based approach\n",
    "2. Fully connected MLP's\n",
    "3. Same as the mask decoder? \n",
    "4. Transformer, ViT? \n",
    "\n",
    "Another idea is to use adapter finetuning ?\n",
    "\n",
    "Furthermore things that can be modified are how many last layers of the SAM encoder are also fine tuned? \n",
    "Possibly just finetune them, after a while of training the newly initialized decoder. \n",
    "\n",
    "We will work with a lr schedule that reduces on plateau.\n",
    "\n",
    "\n",
    "---\n",
    "### Loss function: \n",
    "We use combination of the following: \n",
    "- DiceLoss\n",
    "- FocalLoss\n",
    "- BCEWithLogitsLoss\n",
    "---\n",
    "### Learning rate:\n",
    "- really small values\n",
    "- reduceLROnPlateau(optimizer, 'max', patience=reduce_patience, verbose=verbose, factor=reduce_factor)\n",
    "- cosine annealing\n",
    "\n",
    "\n",
    "TODO: \n",
    "\n",
    "- IMPLEMENT LR SCHEDULE AND STOPPING CRITERIA\n",
    "- CHECK OUT OTHER DECODERS SegFormer, MLP ... \n",
    "- CHECK OUT TO TRAIN MORE LAYERS OF TRANSFORMER\n",
    "\n",
    "# Novel Ideas:\n",
    "\n",
    "- LOOK AT VIT ARCHITECTURE\n",
    "- TRAIN FROM SCRATCH, VARIE SOMETHING WITH LOCAL/GLOBAL ATTENTION or try some COMBINATION RESIDUAL/SKIP CONNECTIONS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Jannek Ulm 16.5.2024\n",
    "# code was inspired by the following sources: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
    "\n",
    "from utils.image_loading import * \n",
    "from utils.torch_device import *\n",
    "from custom_datasets import Sat_Mask_Dataset, Sat_Only_Image_Dataset\n",
    "\n",
    "device = get_torch_device(allow_mps=True)\n",
    "print(\"using device:\",device)\n",
    "\n",
    "###########\n",
    "\n",
    "original_data = {}\n",
    "original_data[\"images\"] =load_training_images()\n",
    "original_data[\"masks\"] = load_groundtruth_images()\n",
    "\n",
    "city_names = [\"boston\",\"nyc\",\"zurich\",\"philadelphia\"]\n",
    "custom_data = {\"images\":[],\"masks\":[]} # stores images and gt masks\n",
    "\n",
    "for name in city_names:\n",
    "    custom_data[\"images\"].extend(load_training_images(name))\n",
    "    custom_data[\"masks\"].extend(load_groundtruth_images(name))\n",
    "\n",
    "custom_data[\"images\"] = custom_data[\"images\"]#[0:200]\n",
    "custom_data[\"masks\"] = custom_data[\"masks\"]#[0:200]\n",
    "\n",
    "assert (len(custom_data[\"images\"]) == len(custom_data[\"masks\"]))\n",
    "\n",
    "\n",
    "print(\"the raw custom dataset contains\",len(custom_data[\"images\"]),\"images\")\n",
    "\n",
    "print(\"custom ds: (min,mean,max) street ratio\",get_street_ratio_mmm(custom_data[\"masks\"]))\n",
    "print(\"orig ds: (min,mean,max) street ratio\",get_street_ratio_mmm(original_data[\"masks\"]))\n",
    "\n",
    "# create a dataset\n",
    "custom_data_set = Sat_Mask_Dataset(custom_data[\"images\"], custom_data[\"masks\"],min_street_ratio=0.03,max_street_ratio=1.0)\n",
    "original_data_set = Sat_Mask_Dataset(original_data[\"images\"],original_data[\"masks\"])\n",
    "print(\"after cleanup, the dataset now contains\",len(custom_data_set),\"images\")\n",
    "\n",
    "\n",
    "# submission kaggle dataset\n",
    "\n",
    "kaggle_submission_images = load_test_images()\n",
    "submission_data_set = Sat_Only_Image_Dataset(kaggle_submission_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = custom_data_set\n",
    "print(dataset[0][0].shape, dataset[0][0].dtype, dataset[0][0].mean(), dataset[0][0].min(), dataset[0][0].max())\n",
    "print(dataset[0][1].shape, dataset[0][1].dtype, dataset[0][1].mean(), dataset[0][1].min(), dataset[0][1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the street ratio distribution of the dataset\n",
    "\n",
    "custom_ratios = get_street_ratio_distr(custom_data[\"masks\"])\n",
    "original_ratios = get_street_ratio_distr(original_data[\"masks\"])\n",
    "\n",
    "plt.hist(custom_ratios,40)\n",
    "plt.hist(original_ratios,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the dataset loading works as planned: \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get some random training images\n",
    "idx = 0\n",
    "image, mask = custom_data_set[idx]\n",
    "\n",
    "img = np.array(image).astype(np.uint8)\n",
    "# swap first and third dimension\n",
    "img = np.swapaxes(img, 0, 2)\n",
    "mask = np.array(mask)\n",
    "mask = np.swapaxes(mask, 0, 2)\n",
    "\n",
    "# 1x2 plot: \n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img)\n",
    "\n",
    "# image + mask overlay\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Image + Mask\")\n",
    "print(\"img: \",img.shape,img.dtype,img.min(),img.max(),img.mean())\n",
    "mask2 = (mask * 255.).astype(np.uint8)\n",
    "print(\"mask2: \",mask2.shape,mask2.dtype,mask2.min(),mask2.max())\n",
    "cmb = img\n",
    "cmb[:,:,0] = mask2[:,:,0]  \n",
    "print(\"cmb: \",cmb.shape,cmb.dtype,cmb.min(),cmb.max(),cmb.mean())\n",
    "plt.imshow(cmb)\n",
    "\n",
    "# mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "print(img.min(), img.max(), mask.min(), mask.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "\n",
    "\n",
    "gpu_batch_size = 1\n",
    "\n",
    "import torch.utils.data\n",
    "custom_train_dataset, custom_test_dataset = torch.utils.data.random_split(custom_data_set, [train_split, 1-train_split])\n",
    "original_train_dataset, original_test_dataset = torch.utils.data.random_split(original_data_set, [train_split, 1-train_split])\n",
    "\n",
    "\n",
    "print(len(custom_train_dataset), len(custom_test_dataset))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# CHECK IF THE FOR MULTIPLE NUM WORKERS THE CODE WORKS AS EXPECTED\n",
    "# SEEMS TO BE CAUSING ISSUES WITH LONG startup and shutdown times for each epoch\n",
    "\n",
    "# decided to drop last to make f1/loss score mean computation easier.\n",
    "\n",
    "#original_train_dataloader = DataLoader(original_train_dataset, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "#original_test_dataloader = DataLoader(original_test_dataset, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "original_dataloader = DataLoader(original_data_set, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# dataloader for submission dataset: \n",
    "submission_dataloader = DataLoader(submission_data_set, batch_size=gpu_batch_size, shuffle=False, drop_last=False,num_workers=4,persistent_workers=True)\n",
    "\n",
    "#custom_train_dataloader = DataLoader(custom_train_dataset, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "#custom_test_dataloader = DataLoader(custom_test_dataset, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "custom_dataloader = DataLoader(custom_data_set, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ (CUSTOM SAM (stored in repo))\n",
    "from custom_segment_anything.segment_anything import sam_model_registry\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import *\n",
    "\n",
    "# load the model from checkpoints on disk:\n",
    "def load_sam_decoder_model_from_checkpoint(encoder_option:int, device,decoder,encoder_finetune_num_last_layers=6):\n",
    "    #\n",
    "    # encoder_option =0 : for vit_b with the encoder that just retunrs the final block output.\n",
    "    # encoder_option =1 : for vit_b with the encoder that returns the intermediate outputs + final output.  \n",
    "\n",
    "    sam_checkpoint_path = \"custom_segment_anything/model_checkpoints/\"\n",
    "    # base, large, huge checkpoints. \n",
    "    checkpoint_names = [\"vit_b\",\"vit_b_intermediate\"]#,\"vit_l\",\"vit_h\"]\n",
    "    checkpoints = [\"sam_vit_b_01ec64.pth\"] #,\"sam_vit_l_0b3195.pth\",\"sam_vit_h_4b8939.pth\"]\n",
    "    model_paths = [sam_checkpoint_path+checkpoint_name for checkpoint_name in checkpoints]\n",
    "    sam = sam_model_registry[checkpoint_names[encoder_option]](checkpoint=model_paths[0])\n",
    "    sam.to(device)\n",
    "    model = SAM_Encoder_Custom_Decoder(sam.preprocess, sam.image_encoder,decoder=decoder,encoder_finetune_num_last_layers=encoder_finetune_num_last_layers)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the finetnued model state, if already started training. \n",
    "def load_finetuned_model(name,device,decoder,encoder_finetune_num_last_layers=6):\n",
    "    model = load_sam_decoder_model_from_checkpoint(0,device,decoder,encoder_finetune_num_last_layers)\n",
    "    finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "    model.load_state_dict(torch.load(finetune_path+name,map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "def load_sam_model(decoder_option, device, encoder_finetune_num_last_layers,finetuned_model_name=\"model.pth\",sam_checkpoint_or_finetuned=\"sam\"):\n",
    "    #   \n",
    "    #   decoder_options: [\"conv\", \"mlp\" , \"segformer_mlp\"]\n",
    "    #   encoder_finetune_num_last_layers tells how many layers of sam encoder are finetuned, all decoder layers are tuned. \n",
    "    #   sam_checkpoint_or_finetuned checks if the model is loaded from a sam checkpoint or a finetuned model with the same architecture.\n",
    "    #\n",
    "\n",
    "    # first construct the model from sam_checkpoint:\n",
    "\n",
    "    if decoder_option == \"conv\":\n",
    "        decoder = Conv_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder.decoder,encoder_finetune_num_last_layers)\n",
    "   \n",
    "    elif decoder_option == \"mlp\":\n",
    "        decoder = MLP_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder,encoder_finetune_num_last_layers)\n",
    "\n",
    "    elif decoder_option == \"spatial-full\":\n",
    "        decoder = MLP_Decoder_Spatially_Aware(context_option=1)\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder,encoder_finetune_num_last_layers)\n",
    "\n",
    "    elif decoder_option == \"spatial-small\":\n",
    "        decoder = MLP_Decoder_Spatially_Aware(context_option=0)\n",
    "        model = load_sam_decoder_model_from_checkpoint(0,device,decoder,encoder_finetune_num_last_layers)\n",
    "    elif decoder_option == \"skip-connect\":\n",
    "        decoder = Skip_MLP_Decoder()\n",
    "        model = load_sam_decoder_model_from_checkpoint(1,device,decoder,encoder_finetune_num_last_layers)\n",
    "    else:\n",
    "        raise ValueError(\"invalid decoder option\")\n",
    "    \n",
    "    # if should load from fine-tuned model, load the model from the finetuned path.\n",
    "    if sam_checkpoint_or_finetuned == \"finetuned\":\n",
    "        finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "        model.load_state_dict(torch.load(finetune_path+finetuned_model_name+\".pth\",map_location=torch.device('cpu')))\n",
    "    elif sam_checkpoint_or_finetuned == \"sam\":\n",
    "        pass\n",
    "        # already initialized model from sam_checkpoint\n",
    "    else: \n",
    "        raise ValueError(\"invalid sam_checkpoint_or_finetuned option\")\n",
    "    \n",
    "    \n",
    "    # Unfreeze last layers of the encoder\n",
    "    for layer_number, param in enumerate(model.sam_encoder.parameters()):\n",
    "        if layer_number > 176 - encoder_finetune_num_last_layers:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze neck of the encoder\n",
    "    model.sam_encoder.neck.requires_grad = True\n",
    "    model.requires_grad = True\n",
    "    print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = \"model_best_so_far.pth\"\n",
    "\n",
    "\n",
    "#model = load_sam_model(\"mlp\",device,encoder_finetune_num_last_layers=0,sam_checkpoint_idx=0,finetuned_model_name=finetuned_model_name,sam_checkpoint_or_finetuned=\"sam\")\n",
    "\n",
    "\n",
    "model = load_sam_model(\"skip-connect\",device,encoder_finetune_num_last_layers=0,finetuned_model_name=finetuned_model_name,sam_checkpoint_or_finetuned=\"sam\")\n",
    "\n",
    "print(\"Model loaded\")\n",
    "# how many trainable parameters does the model have?\n",
    "print(\"Trainable parameters\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# how many total parameters does the model have?\n",
    "print(\"Total parameters\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# percentage of trainable parameters\n",
    "print(\"Percentage of trainable parameters: \")\n",
    "print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "def mean_f1_score_from_logits(pred,mask):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # compute the mean for all the images\n",
    "    # computes the mean over the 0-th axis\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_f1_score_from_classes(mask,pred_classes)\n",
    "\n",
    "\n",
    "def mean_f1_score_from_classes(preds,masks):\n",
    "    \n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # this computes the f1 over the whole batch, for each image in the batch alone:\n",
    "    \n",
    "    # first reshape the tensors\n",
    "    b_size = masks.shape[0]\n",
    "    f1_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i]\n",
    "        pred = preds[i]\n",
    "        # reshape and compute f1\n",
    "        f1_acc = f1_acc + multiclass_f1_score(pred.reshape((size)),mask.reshape((size)))\n",
    "        \n",
    "    mean_f1 = f1_acc/b_size\n",
    "    return mean_f1\n",
    "\n",
    "def dice_loss(logits,masks, smooth=1e-6):\n",
    "    \n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs_flat = probs.reshape(-1)\n",
    "    masks_flat = probs.reshape(-1)\n",
    "    \n",
    "    intersection = (probs_flat * masks_flat).sum()\n",
    "    union = probs_flat.sum() + masks_flat.sum()\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_coeff\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([1./0.13]).to(device)  # Example weights: adjust based on your dataset\n",
    "bce_loss = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "def focal_loss(logits, masks, alpha=0.15, gamma=2.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    loss = sigmoid_focal_loss(probs, masks, alpha=alpha, gamma=gamma, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def combined_loss_1(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return 2 * dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_2(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_3(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = mask.reshape((batch_size,1024*1024))\n",
    "    \n",
    "    return dice_loss(logits, masks, smooth=smooth) + 2 * bce_loss(logits_sq, mask_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now add the logic \n",
    "\n",
    "go over all possible decoder models\n",
    "\n",
    "train for a few epochs with only few encoder layers unlocked until the last epoch didnt improve the original loss\n",
    "increase the number of layers finetuned, \n",
    "if the whole epoch didnt improve, then stop the training in general\n",
    "\n",
    "always store the model, start each new layer round with the best of the last or second last stored model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "train_loader = custom_dataloader\n",
    "test_loader = original_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "max_num_epochs = 3\n",
    "\n",
    "\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader) // 1\n",
    "do_intermed_prints = False\n",
    "\n",
    "grad_batch_size = 5\n",
    "\n",
    "#########\n",
    "\n",
    "decoder_options = [\"skip-connect\",\"spatial-full\",\"spatial-small\"]#[\"conv\", \"mlp\"]\n",
    "num_layers_to_finetune = [25,65,85]\n",
    "learning_rates = [0.001,0.0001, 0.00001]\n",
    "loss_functions = [combined_loss_1,combined_loss_2,combined_loss_3]\n",
    "\n",
    "##################################\n",
    "# TRAINING LOOP\n",
    "################################\n",
    "\n",
    "#for loss_fn_idx in range(len(loss_functions)):\n",
    "loss_fn_idx = 1\n",
    "for decoder_option in decoder_options:\n",
    "    for idx_layer_option in range(len(num_layers_to_finetune)):\n",
    "        for epoch_counter in range(max_num_epochs):\n",
    "            \n",
    "            epoch_to_train = epoch_counter\n",
    "            layer_option = num_layers_to_finetune[idx_layer_option]\n",
    "            learning_rate = learning_rates[idx_layer_option]\n",
    "            loss_fn = loss_functions[loss_fn_idx]\n",
    "            \n",
    "            #####################################            \n",
    "            # now training this model \n",
    "            current_model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option}_epoch_{epoch_counter}\"\n",
    "            print(\"training model:\",current_model_description)\n",
    "            # check if this current model description already exists, if so, load the model and skip this exact training step:\n",
    "            if os.path.exists(\"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\"):\n",
    "                print(\"model already exists, skipping training for this layer option\")\n",
    "                continue\n",
    "            # since model was not already trained\n",
    "            # load the \"start model from checkpoint or finetuned\"\n",
    "            # load the initial model from the sam checkpoint\n",
    "            if idx_layer_option == 0 and epoch_counter == 0:\n",
    "                print(\"loading model from sam checkpoint\")\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=layer_option,finetuned_model_name=None,sam_checkpoint_or_finetuned=\"sam\")\n",
    "            else:\n",
    "                \n",
    "                if epoch_counter == 0:\n",
    "                    # now epoch 0, hence load max epoch from previous layer option\n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{num_layers_to_finetune[idx_layer_option-1]}_epoch_{max_num_epochs-1}\"\n",
    "                else:\n",
    "                    # load the last epoch from current layer option\n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option}_epoch_{epoch_counter-1}\"\n",
    "                print(\"loading model from finetuned:\",model_description)\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=layer_option,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "                \n",
    "            # newly initializing the optimizer and scheduler since model was loaded new (do this for every epoch:)\n",
    "            ####################################\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            print(\"Starting Epoch: \",epoch_counter)\n",
    "            # training run: \n",
    "            model.train()\n",
    "            # store running losses for the epoch and the 10% print interval\n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_f1 = 0.0\n",
    "\n",
    "            short_running_loss = 0.0\n",
    "            short_running_f1 = 0.0\n",
    "\n",
    "            step_counter = 0\n",
    "            \n",
    "            mini_batch_loss_accumulator = 0.0\n",
    "            \n",
    "            # reset the gradients: \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #########################################\n",
    "            # TRAINING LOOP\n",
    "            for image, mask in tqdm(train_loader):\n",
    "                step_counter += 1\n",
    "                #####################\n",
    "                # forward pass\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)    \n",
    "                pred = model(image)\n",
    "                # compute loss and f1 score: \n",
    "                loss = loss_fn(pred,mask)\n",
    "                \n",
    "                \n",
    "                f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                epoch_running_f1 += f1_score\n",
    "                short_running_loss += loss.item()\n",
    "                short_running_f1 += f1_score\n",
    "\n",
    "                mini_batch_loss_accumulator += loss\n",
    "\n",
    "                if do_intermed_prints and step_counter % print_interval == 0:\n",
    "                    print(\"step: \",step_counter//print_interval)\n",
    "                    # print out the current losses:\n",
    "                    print(f\"Epoch: {epoch_counter}, step: {step_counter//print_interval}, (train) Loss: {short_running_loss/print_interval}, F1: {short_running_f1/print_interval}\")\n",
    "                    # and reset the short running losses\n",
    "                    short_running_loss = 0.0\n",
    "                    short_running_f1 = 0.0\n",
    "\n",
    "                ###############\n",
    "                # backward pass\n",
    "                if step_counter % grad_batch_size == 0:\n",
    "                    # compute the gradients\n",
    "                    mini_batch_loss_accumulator = mini_batch_loss_accumulator/grad_batch_size\n",
    "                    mini_batch_loss_accumulator.backward()\n",
    "                    # update the model weights\n",
    "                    optimizer.step()\n",
    "                    # reset the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    mini_batch_loss_accumulator = 0.0\n",
    "                \n",
    "            \n",
    "            print(f\"Epoch: {epoch_counter}, (train) Loss: {epoch_running_loss/len(train_loader)}, F1: {epoch_running_f1/len(train_loader)}\")\n",
    "            ########################################\n",
    "            # save the model in every epoch\n",
    "            print(\"saving model:\",current_model_description)\n",
    "            torch.save(model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\")\n",
    "            #########################################\n",
    "            # testing run: \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                l_acc = 0.0\n",
    "                score_acc = 0.0\n",
    "                for image,mask in tqdm(test_loader):\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    score = mean_f1_score_from_logits(pred,mask)    \n",
    "                    loss =  loss_fn(pred,mask)\n",
    "                    # update running loss and f1 score\n",
    "                    score_acc += score.item()\n",
    "                    l_acc  += loss.item()\n",
    "                    # store the loss and f1 score\n",
    "                print(f\"Epoch: {epoch_counter}, (test) Loss: {l_acc/len(test_loader)}, F1-Score: {score_acc/len(test_loader)}\")    \n",
    "\n",
    "        # save the model after the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to manually finetune each model with another epoch with lower learning rate and more layers to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the generated models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_options = [\"conv\", \"mlp\"]\n",
    "num_layers_to_finetune = [65,85]\n",
    "loss_functions = [combined_loss_1,combined_loss_2,combined_loss_3]\n",
    "range_epochs = [0,1,2]\n",
    "\n",
    "##################################\n",
    "# TESTING LOOP\n",
    "################################\n",
    "\n",
    "eval_d_loader = original_dataloader\n",
    "\n",
    "results_f1 = []\n",
    "results_loss = []\n",
    "\n",
    "for loss_fn_idx in range(len(loss_functions)):\n",
    "    for decoder_option in decoder_options:\n",
    "        for idx_layer_option in range(len(num_layers_to_finetune)):\n",
    "            for epoch_counter in range_epochs:\n",
    "                \n",
    "                layer_option = num_layers_to_finetune[idx_layer_option]\n",
    "\n",
    "                \n",
    "                #####################################            \n",
    "                # now training this model \n",
    "                current_model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option}_epoch_{epoch_counter}\"\n",
    "                print(\"training model:\",current_model_description)\n",
    "                # check if this current model description already exists, if so, load the model and skip this exact training step:\n",
    "                if os.path.exists(\"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\"):\n",
    "                    print(\"model already exists, skipping training for this layer option\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=layer_option,finetuned_model_name=current_model_description,sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "                    \n",
    "                    ### TESTING RUN\n",
    "                    \n",
    "                    model = model.to(device)\n",
    "                    model.eval()\n",
    "                    \n",
    "                    loss_fn = loss_functions[loss_fn_idx]\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        loss_acc = 0.0\n",
    "                        f1_acc = 0.0\n",
    "\n",
    "                        for image,mask in tqdm(eval_d_loader):\n",
    "                            image = image.to(device)\n",
    "                            mask = mask.to(device)\n",
    "                            pred = model(image)\n",
    "                            # compute loss and f1 score: \n",
    "                            score = mean_f1_score_from_logits(pred,mask)\n",
    "                            loss = loss_fn(pred,mask)\n",
    "\n",
    "                            f1_acc += score.item()\n",
    "                            loss_acc  += loss.item() \n",
    "                        print(f\"(test) Loss: {loss_acc/len(eval_d_loader)}, F1-Score: {f1_acc/len(eval_d_loader)}\") \n",
    "                    \n",
    "\n",
    "                    results_f1.append((f1_acc/len(eval_d_loader),current_model_description))\n",
    "                    results_loss.append((loss_acc/len(eval_d_loader),current_model_description))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"could not find model\")\n",
    "                \n",
    "                #\n",
    "###########################\n",
    "# now every models score and loss was stored in the results lists\n",
    "# lets print the best models:\n",
    "\n",
    "results_f1.sort(key=lambda x: x[0],reverse=True)\n",
    "results_loss.sort(key=lambda x: x[0])\n",
    "\n",
    "# print 10 best models and their f1 scores\n",
    "print(\"Best F1 models:\")\n",
    "for i in range(10):\n",
    "    print(results_f1[i])\n",
    "\n",
    "# print 10 best models and their loss scores\n",
    "print(\"Best Loss models:\")\n",
    "for i in range(10):\n",
    "    print(results_loss[i])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we test these models submission:\n",
    "\n",
    "\n",
    "print(\"Best F1 models:\")\n",
    "for i in range(4):\n",
    "    print(results_f1[i])\n",
    "    decoder_option = results_f1[i][1].split(\"_\")[2]\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=0,finetuned_model_name=results_f1[i][1],sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "    model.to(device)\n",
    "    \n",
    "    model_to_submission(model,submission_dataloader,submission_filename = results_f1[i][1]+\"_submission.csv\")\n",
    "#model_to_submission(mlp_model,submission_dataloader,submission_filename =  \"mlp_95_1_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Submission of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"dummy_submission.csv\"):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    all_predictions = []\n",
    "    for model in model_list:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image in tqdm(submission_dataloader):\n",
    "                image = image.to(device)\n",
    "                pred = model(image)\n",
    "                predictions.append(pred)\n",
    "        model = model.to(\"cpu\")\n",
    "    # check the shape of the predictions\n",
    "        assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "        all_predictions.append(predictions)\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    num_models = len(model_list)\n",
    "    num_images = len(all_predictions[0])\n",
    "    assert num_images == 144\n",
    "    \n",
    "    for img_index in range(num_images):\n",
    "        \n",
    "        \n",
    "        ensemble_image = np.zeros((400,400))\n",
    "        \n",
    "        for model_idx in range(num_models):\n",
    "            pred = all_predictions[model_idx][img_index]\n",
    "            pred = pred.squeeze()\n",
    "            # pred is torch vector of shape (1024,1024)\n",
    "            # convert to image\n",
    "            pred = torch.round(torch.sigmoid(pred))\n",
    "            # compress to 400x400\n",
    "            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "            #print(pred.shape)\n",
    "            # pred is now torch vector of shape (1,1,400,400)\n",
    "            # convert to numpy\n",
    "            pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "            #print(pred.shape)\n",
    "            # pred is now numpy vector of shape (400,400)\n",
    "\n",
    "            # add to initial image\n",
    "            ensemble_image += pred\n",
    "        \n",
    "        # now the ensemble image is the sum of all predictions\n",
    "        # need to round and find most common prediction: it should be numbers in range of 0 to num_models\n",
    "        ensemble_image = ensemble_image/num_models\n",
    "        # round the image\n",
    "        if rounding_policy == \"up\":\n",
    "            ensemble_image = np.round(ensemble_image)\n",
    "        elif rounding_policy == \"down\":\n",
    "            ensemble_image = np.floor(ensemble_image)\n",
    "        else:\n",
    "            raise ValueError(\"invalid rounding policy\")\n",
    "        \n",
    "        # store as png to disk\n",
    "        ensemble_image = (ensemble_image * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        ensemble_image = np.stack([ensemble_image,ensemble_image,ensemble_image],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", ensemble_image)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Best F1 models:\")\n",
    "model_list = []\n",
    "for i in range(3):\n",
    "    print(results_f1[i])\n",
    "    decoder_option = results_f1[i][1].split(\"_\")[2]\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_last_layers=0,sam_checkpoint_idx=0,finetuned_model_name=results_f1[i][1],sam_checkpoint_or_finetuned=\"finetuned\")\n",
    "    model = model.to(\"cpu\")\n",
    "    # add model to list\n",
    "    model_list.append(model)\n",
    "    \n",
    "# now ensemble the models and create a submission\n",
    "ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"ensemble_submission_top3_f1_score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and F1 of current model on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN CODE FROM KAGGLE: \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"SAM model + Custom Decoder set to EVAL mode\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sample = original_data_set[10]\n",
    "inpt_0, gt_0 = sample[0], sample[1]\n",
    "with torch.no_grad():\n",
    "    decoder_opt = torch.sigmoid(model(inpt_0.to(device).unsqueeze(0)))\n",
    "print(decoder_opt.shape)\n",
    "\n",
    "decoder_opt_np = ((decoder_opt > 0.5)*1).to(\"cpu\").numpy()[0].transpose(1,2,0)\n",
    "gt_0_np = gt_0.to(\"cpu\").numpy().transpose(1,2,0)\n",
    "print(\"Np arr shape: \", decoder_opt_np.shape, gt_0_np.shape)\n",
    "\n",
    "temp_img_np = inpt_0.to(\"cpu\").numpy()\n",
    "temp_img_np = np.transpose(temp_img_np, [1,2,0])\n",
    "temp_img_np = temp_img_np.astype(np.uint8)\n",
    "\n",
    "# Create a figure with one row and two columns of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "# Display img1 on the first subplot\n",
    "axs[0].imshow(gt_0_np)\n",
    "# Hide the axes of the first subplot\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Display img2 on the second subplot\n",
    "axs[1].imshow(decoder_opt_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Display img3 on the second subplot\n",
    "axs[2].imshow(temp_img_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[2].axis('off')\n",
    "\n",
    "# Adjust the spacing between the subplots\n",
    "fig.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Submission: \n",
    "\n",
    "First run the model over the submission dataloader, generate all the images. Then transform the 1024x1024 images back to the required format and store them on disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "\n",
    "\n",
    "def model_to_submission(model,submission_dataloader,submission_filename =  \"dummy_submission.csv\"):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm(submission_dataloader):\n",
    "            image = image.to(device)\n",
    "            pred = model(image)\n",
    "            predictions.append(pred)\n",
    "    #print(len(predictions), predictions[0].shape)\n",
    "    # check the shape of the predictions\n",
    "    assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    for pred in predictions:\n",
    "        pred = pred.squeeze()\n",
    "        # pred is torch vector of shape (1024,1024)\n",
    "        # convert to image\n",
    "        pred = torch.round(torch.sigmoid(pred))\n",
    "        # compress to 400x400\n",
    "        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "        #print(pred.shape)\n",
    "        # pred is now torch vector of shape (1,1,400,400)\n",
    "        # convert to numpy\n",
    "        pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "        #print(pred.shape)\n",
    "        # pred is now numpy vector of shape (400,400)\n",
    "        # store as png to disk\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        pred = np.stack([pred,pred,pred],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", pred)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_submission(model,submission_dataloader,submission_filename =  \"test_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show images from submission dataset with their prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN CODE FROM KAGGLE: \n",
    "\n",
    "\n",
    "model.eval()\n",
    "print(\"SAM model + Custom Decoder set to EVAL mode\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    " \n",
    "inpt_0 = submission_data_set[2]\n",
    "with torch.no_grad():\n",
    "    decoder_opt = model(inpt_0.to(device).unsqueeze(0))\n",
    "print(decoder_opt.shape)\n",
    "\n",
    "decoder_opt_np = ((decoder_opt > 0.5)*1).to(\"cpu\").numpy()[0].transpose(1,2,0)\n",
    "print(\"Np arr shape: \", decoder_opt_np.shape)\n",
    "\n",
    "temp_img_np = inpt_0.to(\"cpu\").numpy()\n",
    "temp_img_np = np.transpose(temp_img_np, [1,2,0])\n",
    "temp_img_np = temp_img_np.astype(np.uint8)\n",
    "\n",
    "# Create a figure with one row and two columns of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display img2 on the second subplot\n",
    "axs[0].imshow(decoder_opt_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Display img3 on the second subplot\n",
    "axs[1].imshow(temp_img_np)\n",
    "# Hide the axes of the second subplot\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Adjust the spacing between the subplots\n",
    "fig.tight_layout()\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

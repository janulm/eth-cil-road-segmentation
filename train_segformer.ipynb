{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - SegFormer \n",
    "\n",
    "This notebook serves as the training and evaluation procedure to compute a baseline score for SegFormer\n",
    "Check out the SegFormer paper [here](https://arxiv.org/abs/2105.15203)\n",
    "\n",
    "Their code can be found [here](https://github.com/NVlabs/SegFormer?tab=readme-ov-file)\n",
    "\n",
    "The first step is to download a pretrained SegFormer checkpoint as a starting point for training on the gmaps dataset.\n",
    "\n",
    "Please download the ADE20K pretrained model from the [SegFormer Model Zoo](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/xieenze_connect_hku_hk/Ept_oetyUGFCsZTKiL_90kUBy5jmPV65O5rJInsnRCDWJQ?e=CvGohw)\n",
    "\n",
    "\n",
    "For more details please read our [report](BiSeSAM.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n",
      "the raw custom dataset contains 12985 images\n",
      "custom ds: (min,mean,max) street ratio (0.0, 0.13372669666923362, 0.72180625)\n",
      "orig ds: (min,mean,max) street ratio (0.008968750000000001, 0.177976953125, 0.40426875)\n",
      "custom ds with ignore under threshold: (min,mean,max) street ratio (0.03005625, 0.14823332153582142, 0.72180625)\n",
      "Initialzed dataset, checked for min,max street ratio. Discarded %: 0.10242587601078167  num discarded: 1330\n",
      "Initialzed dataset, checked for min,max street ratio. Discarded %: 0.0  num discarded: 0\n",
      "Initialzed dataset, checked for min,max street ratio. Discarded %: 0.0  num discarded: 0\n",
      "after cleanup, the dataset now contains 144 images\n",
      "original dataset contains 11655 images\n",
      "custom dataset contains 144 images\n"
     ]
    }
   ],
   "source": [
    "# written by Jannek Ulm 16.5.2024\n",
    "# code was inspired by the following sources: https://github.com/bnsreenu/python_for_microscopists/blob/master/331_fine_tune_SAM_mito.ipynb\n",
    "\n",
    "from utils.image_loading import * \n",
    "from utils.torch_device import *\n",
    "from custom_datasets import Sat_Mask_Dataset, Sat_Only_Image_Dataset\n",
    "\n",
    "device = get_torch_device(allow_mps=True)\n",
    "print(\"using device:\",device)\n",
    "\n",
    "###### KAGGLE DATA SET #####\n",
    "kaggle_data = {}\n",
    "kaggle_data[\"images\"] =load_training_images()\n",
    "kaggle_data[\"masks\"] = load_groundtruth_images()\n",
    "\n",
    "######## EPFL DATA SET #####\n",
    "epfl_data = {}\n",
    "epfl_data[\"images\"] = load_training_images(\"EPFL\")\n",
    "epfl_data[\"masks\"] = load_groundtruth_images(\"EPFL\")\n",
    "\n",
    "\n",
    "####### CUSTOM GOOGLE MAPS DATA SET #####\n",
    "city_names = [\"boston\",\"nyc\",\"philadelphia\",\"austin\"]\n",
    "gmaps_data = {\"images\":[],\"masks\":[]} # stores images and gt masks\n",
    "for name in city_names:\n",
    "    gmaps_data[\"images\"].extend(load_training_images(name))\n",
    "    gmaps_data[\"masks\"].extend(load_groundtruth_images(name))\n",
    "assert (len(gmaps_data[\"images\"]) == len(gmaps_data[\"masks\"]))\n",
    "\n",
    "\n",
    "######## PRINT GOOGLE MAPS DS STATISTICS ########\n",
    "print(\"the raw custom dataset contains\",len(gmaps_data[\"images\"]),\"images\")\n",
    "print(\"custom ds: (min,mean,max) street ratio\",get_street_ratio_mmm(gmaps_data[\"masks\"]))\n",
    "print(\"orig ds: (min,mean,max) street ratio\",get_street_ratio_mmm(kaggle_data[\"masks\"]))\n",
    "print(\"custom ds with ignore under threshold: (min,mean,max) street ratio\",get_street_ratio_mmm(gmaps_data[\"masks\"],min_ratio_threshold=0.03))\n",
    "\n",
    "\n",
    "##############\n",
    "gmaps_data_set = Sat_Mask_Dataset(gmaps_data[\"images\"], gmaps_data[\"masks\"],min_street_ratio=0.03,max_street_ratio=1.0)\n",
    "kaggle_data_set  = Sat_Mask_Dataset(kaggle_data[\"images\"],kaggle_data[\"masks\"])\n",
    "epfl_data_set = Sat_Mask_Dataset(epfl_data[\"images\"],epfl_data[\"masks\"])\n",
    "print(\"after cleanup, the dataset now contains\",len(kaggle_data_set),\"images\")\n",
    "\n",
    "######## KAGGLE - SUBMISSION DATA SET ########\n",
    "kaggle_submission_images = load_test_images()\n",
    "kaggle_submission_data_set = Sat_Only_Image_Dataset(kaggle_submission_images)\n",
    "\n",
    "# print num of data in original and custom dataset\n",
    "print(\"original dataset contains\",len(gmaps_data_set),\"images\")\n",
    "print(\"custom dataset contains\",len(kaggle_data_set),\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "gpu_batch_size = 1\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "torch.manual_seed(0)\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0)\n",
    "\n",
    "kaggle_train_dataset, kaggle_test_dataset = torch.utils.data.random_split(kaggle_data_set, [train_split, 1-train_split],generator=gen)\n",
    "gmaps_train_dataset, gmaps_test_dataset = torch.utils.data.random_split(gmaps_data_set, [train_split, 1-train_split],generator=gen)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "gmaps_dataloader = DataLoader(gmaps_data_set, batch_size=1, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# kaggle dataloader\n",
    "kaggle_dataloader = DataLoader(kaggle_data_set, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "kaggle_train_dataloader = DataLoader(kaggle_train_dataset, batch_size=gpu_batch_size, shuffle=True, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "kaggle_test_dataloader = DataLoader(kaggle_test_dataset, batch_size=gpu_batch_size, shuffle=False, drop_last=True,num_workers=4,persistent_workers=True)\n",
    "\n",
    "# dataloader for submission dataset: \n",
    "submission_dataloader = DataLoader(kaggle_submission_data_set, batch_size=1, shuffle=False, drop_last=False,num_workers=4,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SegFormer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janulm/miniconda3/envs/mps/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SegformerImageProcessor, SegformerForSemanticSegmentation\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/utils/import_utils.py:1501\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1500\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1501\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/utils/import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1500\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/utils/import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor, BatchFeature, get_size_dict\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize, to_channel_dimension_format\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     IMAGENET_DEFAULT_MEAN,\n\u001b[1;32m     26\u001b[0m     IMAGENET_DEFAULT_STD,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     validate_preprocess_arguments,\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/image_processing_utils.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature \u001b[38;5;28;01mas\u001b[39;00m BaseBatchFeature\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     IMAGE_PROCESSOR_NAME,\n\u001b[1;32m     32\u001b[0m     PushToHubMixin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     logging,\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/transformers/image_transforms.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/__init__.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mps/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 150, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################\n",
    "\n",
    "def load_segformer(device, encoder_finetune_num_first_layers,encoder_finetune_num_last_layers,finetuned_model_name=\"model.pth\",segformer_checkpoint_or_finetuned=\"segformer\",load_from_compiled=False):\n",
    "    #   \n",
    "    #   \n",
    "    #\n",
    "    # first construct the model from sam_checkpoint:\n",
    "\n",
    "    \n",
    "    # if should load from fine-tuned model, load the model from the finetuned path.\n",
    "    if segformer_checkpoint_or_finetuned == \"finetuned\":\n",
    "        finetune_path = \"custom_segment_anything/model_checkpoints/finetuned/\"\n",
    "        \n",
    "        if \"custom_segment_anything/model_checkpoints\" in finetuned_model_name:\n",
    "            load_path = finetuned_model_name+\".pth\"\n",
    "        else:\n",
    "            load_path = finetune_path+finetuned_model_name+\".pth\"\n",
    "        if load_from_compiled:\n",
    "            model = torch.compile(model)\n",
    "        model.load_state_dict(torch.load(load_path,map_location=torch.device('cpu')))\n",
    "    elif segformer_checkpoint_or_finetuned == \"sam\":\n",
    "        pass\n",
    "        # already initialized model from sam_checkpoint\n",
    "    else: \n",
    "        raise ValueError(\"invalid segformer_checkpoint_or_finetuned option\")\n",
    "    \n",
    "    # Unfreeze last layers of the encoder\n",
    "    for layer_number, param in enumerate(model.sam_encoder.parameters()):\n",
    "        if layer_number > 176 - encoder_finetune_num_last_layers or layer_number < encoder_finetune_num_first_layers:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze neck of the encoder\n",
    "    model.sam_encoder.neck.requires_grad = True\n",
    "    #model.requires_grad = True\n",
    "    print(\"Percentage of trainable parameters: \",sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "def mean_f1_score_from_logits(pred,mask):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # compute the mean for all the images\n",
    "    # computes the mean over the 0-th axis\n",
    "    pred_classes = torch.round(torch.sigmoid(pred))\n",
    "    return mean_f1_score_from_classes(mask,pred_classes)\n",
    "\n",
    "\n",
    "def mean_f1_score_from_classes(preds,masks):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # this computes the f1 over the whole batch, for each image in the batch alone:\n",
    "    # first reshape the tensors\n",
    "    b_size = masks.shape[0]\n",
    "    f1_acc = 0.\n",
    "    size = torch.prod(torch.tensor(masks.shape[1:]))\n",
    "    for i in range(b_size):\n",
    "        mask = masks[i]\n",
    "        pred = preds[i]\n",
    "        # reshape and compute f1\n",
    "        f1_acc = f1_acc + multiclass_f1_score(pred.reshape((size)),mask.reshape((size)))\n",
    "        \n",
    "    mean_f1 = f1_acc/b_size\n",
    "    return mean_f1\n",
    "\n",
    "def dice_loss(logits,masks, smooth=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs_flat = probs.reshape(-1)\n",
    "    masks_flat = masks.reshape(-1)\n",
    "    \n",
    "    intersection = (probs_flat * masks_flat).sum()\n",
    "    union = probs_flat.sum() + masks_flat.sum()\n",
    "    dice_coeff = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice_coeff\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([1./0.13]).to(device)  # Example weights: adjust based on your dataset\n",
    "bce_loss = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "def focal_loss(logits, masks, alpha=0.15, gamma=2.0):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    loss = sigmoid_focal_loss(probs, masks, alpha=alpha, gamma=gamma, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def only_bce(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = masks.reshape((batch_size,1024*1024))\n",
    "    return bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def combined_loss_dice(logits, masks, alpha=0.15, gamma=2.0, smooth=1e-6):\n",
    "    # reshape the mask and predictions for the bce loss: \n",
    "    batch_size = logits.shape[0]\n",
    "    logits_sq = logits.reshape((batch_size,1024*1024))\n",
    "    mask_sq = masks.reshape((batch_size,1024*1024))\n",
    "    return dice_loss(logits, masks, smooth=smooth) + bce_loss(logits_sq, mask_sq)\n",
    "\n",
    "def get_confusion_values(logits,masks):\n",
    "    # assume both inputs have the shape (batchsize,1, h,w), where h=w=1024\n",
    "    # this computes the f1 over the whole batch, for each image in the batch alone:\n",
    "    # first reshape the tensors\n",
    "    pred = torch.round(torch.sigmoid(logits))\n",
    "    \n",
    "    tp = torch.sum(pred * masks)\n",
    "    tn = torch.sum((1 - pred) * (1 - masks))\n",
    "    fp = torch.sum(pred * (1 - masks))\n",
    "    fn = torch.sum((1 - pred) * masks)\n",
    "    return tp,tn,fp,fn\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - gmaps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEAR ALL CUDA MEMORY\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "train_loader = gmaps_dataloader\n",
    "test_loader = kaggle_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader)\n",
    "20\n",
    "do_intermed_prints = False\n",
    "\n",
    "#########\n",
    "\n",
    "decoder_options =  [\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\",\"mlp\"]\n",
    "num_layers_to_finetune_back = [25,65,85,105]\n",
    "num_layers_to_finetune_front = [0,0,15,25]\n",
    "learning_rates = [0.001,0.0001, 0.0001,0.0001]\n",
    "loss_functions = [only_bce,combined_loss_dice,combined_loss_dice]\n",
    "max_epochs = [3,3,4,4]\n",
    "grad_batch_size_choices = [5,5,5,5]\n",
    "\n",
    "##################################\n",
    "# TRAINING LOOP\n",
    "################################\n",
    "\n",
    "# OPTIMIZATIONS \n",
    "mean_f1_score_from_logits = torch.compile(mean_f1_score_from_logits)\n",
    "loss_fn_idx = 2\n",
    "# Hyperparameter tuning yielded that we use the loss function BCE + Dice for the best results\n",
    "\n",
    "loss_fn = loss_functions[loss_fn_idx]\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "print(\"using loss function:\",loss_fn)\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "        \n",
    "    for idx_layer_option in range(len(num_layers_to_finetune_back)):\n",
    "        # set the max number of epochs for this layer option\n",
    "        max_num_epochs = max_epochs[idx_layer_option]\n",
    "        \n",
    "        for epoch_counter in range(max_num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            epoch_to_train = epoch_counter\n",
    "            layer_option_back = num_layers_to_finetune_back[idx_layer_option]\n",
    "            layer_option_front = num_layers_to_finetune_front[idx_layer_option]\n",
    "            \n",
    "            # if the layer_option is larger than 1 (i.e. if at least 85 layers in the back are tuned, reduce lr.), reduce lr \n",
    "            learning_rate = learning_rates[idx_layer_option]\n",
    "            if idx_layer_option >= 2:\n",
    "                learning_rate = learning_rate / (epoch_counter + 1)\n",
    "                print(\"divided lr\")\n",
    "            print(\"using lr:\",learning_rate)\n",
    "            #loss_fn = loss_functions[loss_fn_idx]\n",
    "            #print(\"using loss function:\",loss_fn)\n",
    "            grad_batch_size = grad_batch_size_choices[loss_fn_idx]\n",
    "            \n",
    "            #####################################            \n",
    "            # now training this model \n",
    "            current_model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option_front}_{layer_option_back}_epoch_{epoch_counter}\"\n",
    "            print(\"training model:\",current_model_description)\n",
    "            # check if this current model description already exists, if so, load the model and skip this exact training step:\n",
    "            if os.path.exists(\"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\"):\n",
    "                print(\"model already exists, skipping training for this layer option\")\n",
    "                continue\n",
    "            # since model was not already trained\n",
    "            # load the \"start model from checkpoint or finetuned\"\n",
    "            # load the initial model from the sam checkpoint\n",
    "            if idx_layer_option == 0 and epoch_counter == 0:\n",
    "                print(\"loading model from sam checkpoint\")\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=layer_option_front,encoder_finetune_num_last_layers=layer_option_back,finetuned_model_name=None,sam_checkpoint_or_finetuned=\"sam\",load_from_compiled=False)\n",
    "                model = torch.compile(model)\n",
    "            else:\n",
    "                \n",
    "                if epoch_counter == 0:\n",
    "                    last_max_epoch = max_epochs[idx_layer_option-1]\n",
    "                    # now epoch 0, hence load max epoch from previous layer option\n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{num_layers_to_finetune_front[idx_layer_option-1]}_{num_layers_to_finetune_back[idx_layer_option-1]}_epoch_{last_max_epoch-1}\"\n",
    "                else:\n",
    "                    # load the last epoch from current layer option\n",
    "                    \n",
    "                    model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{layer_option_front}_{layer_option_back}_epoch_{epoch_counter-1}\"\n",
    "                print(\"loading model from finetuned:\",model_description)\n",
    "                model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=layer_option_front,encoder_finetune_num_last_layers=layer_option_back,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "                \n",
    "            # newly initializing the optimizer and scheduler since model was loaded new (do this for every epoch:)\n",
    "            ####################################\n",
    "            model.to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            print(\"Starting Epoch: \",epoch_counter)\n",
    "            # training run: \n",
    "            model.train()\n",
    "            # store running losses for the epoch and the 10% print interval\n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_f1 = 0.0\n",
    "\n",
    "            short_running_loss = 0.0\n",
    "            short_running_f1 = 0.0\n",
    "\n",
    "            step_counter = 0\n",
    "            \n",
    "            # reset the gradients: \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #########################################\n",
    "            # TRAINING LOOP\n",
    "            for image, mask in tqdm(train_loader):\n",
    "                step_counter += 1\n",
    "                #####################\n",
    "                # forward pass\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)    \n",
    "                pred = model(image)\n",
    "                # compute loss and f1 score: \n",
    "                loss = loss_fn(pred,mask)\n",
    "                \n",
    "                \n",
    "                f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                epoch_running_f1 += f1_score\n",
    "                short_running_loss += loss.item()\n",
    "                short_running_f1 += f1_score\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                ###############\n",
    "                # backward pass\n",
    "                if step_counter % grad_batch_size == 0:\n",
    "                    # update the model weights\n",
    "                    optimizer.step()\n",
    "                    # reset the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                if do_intermed_prints and step_counter % print_interval == 0:\n",
    "                    print(\"step: \",step_counter//print_interval)\n",
    "                    # print out the current losses:\n",
    "                    print(f\"Epoch: {epoch_counter}, step: {step_counter//print_interval}, (train) Loss: {short_running_loss/print_interval}, F1: {short_running_f1/print_interval}\")\n",
    "                    # and reset the short running losses\n",
    "                    short_running_loss = 0.0\n",
    "                    short_running_f1 = 0.0\n",
    "\n",
    "            print(f\"Epoch: {epoch_counter}, (train) Loss: {epoch_running_loss/len(train_loader)}, F1: {epoch_running_f1/len(train_loader)}\")\n",
    "            ########################################\n",
    "            # save the model in every epoch\n",
    "            print(\"saving model:\",current_model_description)\n",
    "            torch.save(model.state_dict(), \"custom_segment_anything/model_checkpoints/finetuned/\"+current_model_description+\".pth\")\n",
    "            #########################################\n",
    "            \n",
    "            # testing run: \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                l_acc = 0.0\n",
    "                score_acc = 0.0\n",
    "                for image,mask in tqdm(test_loader):\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    score = mean_f1_score_from_logits(pred,mask)    \n",
    "                    loss =  loss_fn(pred,mask)\n",
    "                    # update running loss and f1 score\n",
    "                    score_acc += score.item()\n",
    "                    l_acc  += loss.item()\n",
    "                    # store the loss and f1 score\n",
    "                print(f\"Epoch: {epoch_counter}, (test) Loss: {l_acc/len(test_loader)}, F1-Score: {score_acc/len(test_loader)}\")\n",
    "        # save the model after the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pre-Training Performance\n",
    "\n",
    "Now, check the performance on the kaggle dataset of the last model trained for each decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "print(len(kaggle_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of trainable parameters:  0.14234610224075397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 0 Epoch: 0, (test) Loss: 2.3932500119720186, F1-Score: 0.8987670647246497\n",
      "Percentage of trainable parameters:  0.14234610224075397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 0 Epoch: 1, (test) Loss: 2.2579501271247864, F1-Score: 0.9049292973109654\n",
      "Percentage of trainable parameters:  0.14234610224075397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 0 Epoch: 2, (test) Loss: 2.0169496472392763, F1-Score: 0.9156574543033328\n",
      "Percentage of trainable parameters:  0.35300521010298497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 1 Epoch: 0, (test) Loss: 1.956272263612066, F1-Score: 0.9182838712419782\n",
      "Percentage of trainable parameters:  0.35300521010298497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 1 Epoch: 1, (test) Loss: 1.7879834324121475, F1-Score: 0.9258792379072734\n",
      "Percentage of trainable parameters:  0.35300521010298497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 1 Epoch: 2, (test) Loss: 1.8384676980120795, F1-Score: 0.92501848084586\n",
      "Percentage of trainable parameters:  0.559055504426283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 2 Epoch: 0, (test) Loss: 1.8218104818037577, F1-Score: 0.9252932092973164\n",
      "Percentage of trainable parameters:  0.559055504426283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 2 Epoch: 1, (test) Loss: 1.6634620087487357, F1-Score: 0.9313207417726517\n",
      "Percentage of trainable parameters:  0.559055504426283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 2 Epoch: 2, (test) Loss: 1.7121892614024026, F1-Score: 0.9293348469904491\n",
      "Percentage of trainable parameters:  0.559055504426283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 2 Epoch: 3, (test) Loss: 1.7406296453305654, F1-Score: 0.9295554842267718\n",
      "Percentage of trainable parameters:  0.710502399520313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 3 Epoch: 0, (test) Loss: 1.7816519801105772, F1-Score: 0.9263068245989936\n",
      "Percentage of trainable parameters:  0.710502399520313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 3 Epoch: 1, (test) Loss: 1.6899396010807581, F1-Score: 0.9315016929592405\n",
      "Percentage of trainable parameters:  0.710502399520313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 3 Epoch: 2, (test) Loss: 1.6597330953393663, F1-Score: 0.9318761165652957\n",
      "Percentage of trainable parameters:  0.710502399520313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder: conv,Loss Function 2: Layer_idx: 3 Epoch: 3, (test) Loss: 1.6617595553398132, F1-Score: 0.9329096909080233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "decoder_options =  [\"conv\"] #,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\",\"mlp\"]\n",
    "## select the dataloader\n",
    "test_loader = kaggle_test_dataloader\n",
    "\n",
    "losses = {}\n",
    "f1_scores = {}\n",
    "\n",
    "layers_front = [0,0,15,25]\n",
    "layers_back = [25,65,85,105]\n",
    "epoch_ranges = [3,3,4,4]\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "loss_fn_idx = 2\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    losses[decoder_option] = []\n",
    "    f1_scores[decoder_option] = []\n",
    "    \n",
    "    for layer_idx in range(len(layers_front)):\n",
    "        epoch_range = epoch_ranges[layer_idx]\n",
    "        for epoch_counter in range(epoch_range):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            l_front = layers_front[layer_idx]\n",
    "            l_back = layers_back[layer_idx]\n",
    "            \n",
    "            model_description = f\"model_{loss_fn_idx+1}_{decoder_option}_decoder_finetune_last_{l_front}_{l_back}_epoch_{epoch_counter}\"\n",
    "            model_path = \"custom_segment_anything/model_checkpoints/finetuned/\"+model_description\n",
    "            compiled = (decoder_option != \"mlp\") # only the mlp was trained with the non compiled option initially\n",
    "            model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=l_front,encoder_finetune_num_last_layers=l_back,finetuned_model_name=model_path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=compiled)\n",
    "            \n",
    "            # for each model compute the f1 and loss score of the original dataset\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                l_acc = 0.0\n",
    "                score_acc = 0.0\n",
    "                for image,mask in tqdm(test_loader):\n",
    "                    image = image.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(image)\n",
    "                    # compute loss and f1 score: \n",
    "                    score = mean_f1_score_from_logits(pred,mask)    \n",
    "                    loss =  loss_fn(pred,mask)\n",
    "                    # update running loss and f1 score\n",
    "                    score_acc += score.item()\n",
    "                    l_acc  += loss.item()\n",
    "                    # store the loss and f1 score\n",
    "                    \n",
    "                mean_f1 = score_acc/len(test_loader)\n",
    "                mean_loss = l_acc/len(test_loader)\n",
    "                losses[decoder_option].append(mean_loss)\n",
    "                f1_scores[decoder_option].append(mean_f1)\n",
    "                print(f\"Decoder: {decoder_option},Loss Function {loss_fn_idx}: Layer_idx: {layer_idx} Epoch: {epoch_counter}, (test) Loss: {mean_loss}, F1-Score: {mean_f1}\")\n",
    "            \n",
    "# plot the resolts for all loss function in two seperate plots, one for f1 and one for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv': [0.8992442041635513, 0.9047977945634297, 0.9151669549090522, 0.9197393442903247, 0.9250707647630146, 0.9245244456189019, 0.9265367963484356, 0.9312025138310024, 0.9294231959751674, 0.9299504097018924, 0.9284700304269791, 0.9325949741261346, 0.9325963088444301, 0.9318815980638776]}\n"
     ]
    }
   ],
   "source": [
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data\n",
    "epoch_ranges = [4, 5, 5]  # Assuming these are the epoch ranges, replace with actual values if different\n",
    "epoch_sum = sum(epoch_ranges)\n",
    "epochs = list(range(epoch_sum))\n",
    "\n",
    "f1_scores = {'conv': [0.8992442041635513, 0.9047977945634297, 0.9151669549090522, 0.9197393442903247, 0.9250707647630146, 0.9245244456189019, 0.9265367963484356, 0.9312025138310024, 0.9294231959751674, 0.9299504097018924, 0.9284700304269791, 0.9325949741261346, 0.9325963088444301, 0.9318815980638776]}\n",
    "trainable_params = {'conv': [0.142, 0.142, 0.142, 0.3530, 0.3530, 0.3530, 0.559, 0.559, 0.559, 0.559, 0.7105, 0.7105, 0.7105, 0.7105]}\n",
    "\n",
    "decoder_options = ['conv']  # Assuming 'conv' is the only decoder option, add more if needed\n",
    "\n",
    "# Plot F1 Scores\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    ax1.plot(epochs, f1_scores[decoder_option], label=f'BiSeSAM-Conv F1 Score')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Kaggle Test F1 Score')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# add a second y-axis for the trainable parameters\n",
    "ax2 = ax1.twinx()\n",
    "for decoder_option in decoder_options:\n",
    "    ax2.plot(epochs, trainable_params[decoder_option], label=f'BiSeSAM-Conv Ratio Trainable Params', linestyle='dashed', color='red')\n",
    "ax2.set_ylabel('Ratio of trainable Parameters')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "plt.title('Conv - BiSeSAM - F1 Score and Trainable Parameters over Epochs for different Decoders')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out the best finetuning strategy:\n",
    "\n",
    "for this try out different learning rate and dataset options. Tune for at most 20 epochs, get the scores and validate them in the kaggle submission system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# ENABLE TF32 compute precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "lr_options = [0.0001,0.00001]\n",
    "\n",
    "training_options = [kaggle_train_dataloader]\n",
    "test_options = [kaggle_test_dataloader]\n",
    "\n",
    "training_desc = [\"kaggle\"]\n",
    "\n",
    "# now check only for the best model, the mlp model\n",
    "\n",
    "\n",
    "max_epochs = 21\n",
    "\n",
    "train_losses = {}\n",
    "train_f1_scores = {}\n",
    "test_losses = {}\n",
    "test_f1_scores = {}\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "\n",
    "for lr in lr_options:\n",
    "    for train_idx in range(len(training_desc)):\n",
    "        train_loader = training_options[train_idx]\n",
    "        test_loader = test_options[train_idx]\n",
    "        \n",
    "        desc_key = f\"lr_{lr}_{training_desc[train_idx]}\"\n",
    "        \n",
    "        print(\"now doing: \",desc_key)\n",
    "        \n",
    "        train_losses[desc_key] = []\n",
    "        train_f1_scores[desc_key] = []\n",
    "        test_losses[desc_key] = []\n",
    "        test_f1_scores[desc_key] = []\n",
    "        \n",
    "        model = load_sam_model(\"mlp\",device,encoder_finetune_num_first_layers=25,encoder_finetune_num_last_layers=105,finetuned_model_name=\"model_3_mlp_decoder_finetune_last_25_105_epoch_3\",sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=False)\n",
    "        model = torch.compile(model)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            # TRAIN\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_running_loss = 0.0\n",
    "            epoch_running_f1 = 0.0\n",
    "            step_counter = 0\n",
    "            for img,mask in tqdm(train_loader):\n",
    "                step_counter += 1\n",
    "                img = img.to(device)\n",
    "                mask = mask.to(device)\n",
    "                pred = model(img)\n",
    "                loss = loss_fn(pred,mask)\n",
    "                f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                epoch_running_f1 += f1_score\n",
    "                loss.backward()\n",
    "                if step_counter % 5 == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            # save the results: \n",
    "            train_losses[desc_key].append(epoch_running_loss/len(train_loader))\n",
    "            train_f1_scores[desc_key].append(epoch_running_f1/len(train_loader))    \n",
    "            \n",
    "            # save the model at specific intervals: \n",
    "            if epoch % 5 == 0:\n",
    "                model_path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/mlp_{desc_key}_epoch_{epoch}.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # TEST\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_running_loss = 0.0\n",
    "                epoch_running_f1 = 0.0\n",
    "                step_counter = 0\n",
    "                for img,mask in tqdm(test_loader):\n",
    "                    step_counter += 1\n",
    "                    img = img.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    pred = model(img)\n",
    "                    loss = loss_fn(pred,mask)\n",
    "                    f1_score = mean_f1_score_from_logits(pred,mask).item()\n",
    "                    epoch_running_loss += loss.item()\n",
    "                    epoch_running_f1 += f1_score\n",
    "                    \n",
    "                # save the results: \n",
    "                test_losses[desc_key].append(epoch_running_loss/len(test_loader))\n",
    "                test_f1_scores[desc_key].append(epoch_running_f1/len(test_loader))    \n",
    "\n",
    "\n",
    "print(\"train losses:\",train_losses)\n",
    "print(\"train f1 scores:\",train_f1_scores)\n",
    "print(\"test losses:\",test_losses)\n",
    "print(\"test f1 scores:\",test_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each key in test_f1_scores; print the 0,5,10,15,20 epoch results\n",
    "\n",
    "max_thing = (0,0)\n",
    "max_val = 0.\n",
    "\n",
    "for key in test_f1_scores.keys():\n",
    "    print(f\"key: {key}\")\n",
    "    idxs = [0,5,10,15,20]\n",
    "    for idx in idxs:\n",
    "        print(f\"epoch: {idx}, loss: {test_losses[key][idx]}, f1: {test_f1_scores[key][idx]}\")\n",
    "        if test_f1_scores[key][idx] > max_val:\n",
    "            max_val = test_f1_scores[key][idx]\n",
    "            max_thing = (key,idx)\n",
    "            \n",
    "print(\"max thing:\",max_thing, \"max val:\",max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation reasons I want to submit one model prediction to the kaggle submission system:\n",
    "\n",
    "lr_1e-05_kaggle, 15, local F1: 94.06 -> Online: 93.253\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune each model with the kaggle data\n",
    "\n",
    "The finetuning strategy will be\n",
    "\n",
    "batch_size = 15\n",
    "run for at most 15 epochs\n",
    "store for each decoder option, the best achieving f1 and score model seperately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### CLEAR ALL CUDA MEMORY\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "train_loader = kaggle_train_dataloader\n",
    "test_loader = kaggle_test_dataloader\n",
    "\n",
    "\n",
    "########\n",
    "# print interval and num_steps for gradient accumulation\n",
    "print_interval = len(train_loader) // 10\n",
    "do_intermed_prints = False\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    "\n",
    "max_epochs = 15\n",
    "\n",
    "##################################\n",
    "# TRAINING LOOP\n",
    "################################\n",
    "\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    \n",
    "    # load the current model: \n",
    "    load_compiled = (decoder_option != \"mlp\")\n",
    "    model_description = f\"model_3_{decoder_option}_decoder_finetune_last_25_105_epoch_3\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=25,encoder_finetune_num_last_layers=105,finetuned_model_name=model_description,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=load_compiled)\n",
    "    if not load_compiled:\n",
    "        model = torch.compile(model)\n",
    "    best_f1 = 0.\n",
    "    best_loss = 1000.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # training run: \n",
    "        model.train()\n",
    "        # reset the gradients: \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #########################################\n",
    "        # TRAINING LOOP\n",
    "        step_counter = 0\n",
    "        for image, mask in tqdm(train_loader):\n",
    "            step_counter += 1\n",
    "            #####################\n",
    "            # forward pass\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)    \n",
    "            pred = model(image)\n",
    "            # compute loss and f1 score: \n",
    "            loss = loss_fn(pred,mask)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            ###############\n",
    "            # backward pass\n",
    "            if step_counter % 5 == 0:\n",
    "                # compute the gradients\n",
    "                # update the model weights\n",
    "                optimizer.step()\n",
    "                # reset the gradients\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        ####### EVALUATION\n",
    "        \n",
    "        \n",
    "        # testing run: \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            l_acc = 0.0\n",
    "            score_acc = 0.0\n",
    "            for image,mask in tqdm(test_loader):\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "                pred = model(image)\n",
    "                # compute loss and f1 score: \n",
    "                score = mean_f1_score_from_logits(pred,mask)    \n",
    "                loss =  loss_fn(pred,mask)\n",
    "                # update running loss and f1 score\n",
    "                score_acc += score.item()\n",
    "                l_acc  += loss.item()\n",
    "                # store the loss and f1 score\n",
    "                \n",
    "            mean_f1 = score_acc/len(test_loader)\n",
    "            mean_loss = l_acc/len(test_loader)\n",
    "        \n",
    "        save_model_description_f1  = f\"decoder_{decoder_option}_bestf1.pth\"\n",
    "        save_model_description_loss = f\"decoder_{decoder_option}_bestloss.pth\"\n",
    "        file_path = \"custom_segment_anything/model_checkpoints/finetuned_kaggle/\"\n",
    "        \n",
    "        if mean_f1 > best_f1:\n",
    "            best_f1 = mean_f1\n",
    "            torch.save(model.state_dict(), file_path+save_model_description_f1)\n",
    "            print(\"Decoder option:\",decoder_option,\"new best f1:\",best_f1)\n",
    "        \n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(model.state_dict(), file_path+save_model_description_loss)\n",
    "            print(\"Decoder option:\",decoder_option,\"new best loss:\",best_loss)\n",
    "            \n",
    "        #########################################\n",
    "        \n",
    "    print(\"finished training for decoder option:\",decoder_option)\n",
    "    print(\"best f1:\",best_f1, \"best loss:\",best_loss)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finished training for decoder option: mlp\n",
    "best f1: 0.9406011423894337 best loss: 1.4403232016733714\n",
    "\n",
    "finished training for decoder option: conv\n",
    "best f1: 0.94107914183821 best loss: 1.443434430020196\n",
    "\n",
    "finished training for decoder option: spatial-small\n",
    "best f1: 0.9401421227625438 best loss: 1.4749739553247179\n",
    "\n",
    "finished training for decoder option: spatial-full\n",
    "best f1: 0.9409123829432896 best loss: 1.432130630527224\n",
    "\n",
    "finished training for decoder option: skip-connect\n",
    "best f1: 0.9407457964760917 best loss: 1.4466931287731444\n",
    "\n",
    "# Single Model Submission and Rounding Ensemble Submission of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "\n",
    "\n",
    "def model_to_submission(model,submission_dataloader,submission_filename =  \"dummy_submission.csv\"):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image in tqdm(submission_dataloader):\n",
    "            image = image.to(device)\n",
    "            pred = model(image)\n",
    "            predictions.append(pred)\n",
    "    #print(len(predictions), predictions[0].shape)\n",
    "    # check the shape of the predictions\n",
    "    assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    for pred in predictions:\n",
    "        pred = pred.squeeze()\n",
    "        # pred is torch vector of shape (1024,1024)\n",
    "        # convert to image\n",
    "        pred = torch.round(torch.sigmoid(pred))\n",
    "        # compress to 400x400\n",
    "        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "        #print(pred.shape)\n",
    "        # pred is now torch vector of shape (1,1,400,400)\n",
    "        # convert to numpy\n",
    "        pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "        #print(pred.shape)\n",
    "        # pred is now numpy vector of shape (400,400)\n",
    "        # store as png to disk\n",
    "        pred = (pred * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        pred = np.stack([pred,pred,pred],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", pred)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir)\n",
    "\n",
    "\n",
    "def ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"dummy_submission.csv\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    all_predictions = []\n",
    "    for model in model_list:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image in tqdm(submission_dataloader):\n",
    "                image = image.to(device)\n",
    "                pred = model(image)\n",
    "                predictions.append(pred)\n",
    "        model = model.to(\"cpu\")\n",
    "    # check the shape of the predictions\n",
    "        assert (torch.tensor(predictions[0].shape) == torch.tensor([1, 1, 1024, 1024])).min().item()\n",
    "        all_predictions.append(predictions)\n",
    "    # go over all predictions and convert the logits to predictions:\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # go over all predictions, convert them and store in disk:\n",
    "    counter = 144\n",
    "    num_models = len(model_list)\n",
    "    num_images = len(all_predictions[0])\n",
    "    assert num_images == 144\n",
    "    \n",
    "    for img_index in range(num_images):\n",
    "        \n",
    "        \n",
    "        ensemble_image = np.zeros((400,400))\n",
    "        \n",
    "        for model_idx in range(num_models):\n",
    "            pred = all_predictions[model_idx][img_index]\n",
    "            pred = pred.squeeze()\n",
    "            # pred is torch vector of shape (1024,1024)\n",
    "            # convert to image\n",
    "            pred = torch.round(torch.sigmoid(pred))\n",
    "            # compress to 400x400\n",
    "            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(400,400), mode='nearest')\n",
    "            #print(pred.shape)\n",
    "            # pred is now torch vector of shape (1,1,400,400)\n",
    "            # convert to numpy\n",
    "            pred = pred.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "            #print(pred.shape)\n",
    "            # pred is now numpy vector of shape (400,400)\n",
    "\n",
    "            # add to initial image\n",
    "            ensemble_image += pred\n",
    "        \n",
    "        # now the ensemble image is the sum of all predictions\n",
    "        # need to round and find most common prediction: it should be numbers in range of 0 to num_models\n",
    "        if rounding_policy == \"up\" or rounding_policy == \"down\":\n",
    "            ensemble_image = ensemble_image/num_models\n",
    "        \n",
    "            # round the image\n",
    "            if rounding_policy == \"up\":\n",
    "                ensemble_image = np.round(ensemble_image)\n",
    "            elif rounding_policy == \"down\":\n",
    "                ensemble_image = np.floor(ensemble_image)\n",
    "            else:\n",
    "                raise ValueError(\"invalid rounding policy\")\n",
    "        elif rounding_policy == \"min1\":\n",
    "            ensemble_image[ensemble_image > 1] = 1\n",
    "            #ensemble_image = np.round(ensemble_image)\n",
    "        else:\n",
    "            raise ValueError(\"invalid rounding policy\")\n",
    "        # store as png to disk\n",
    "        ensemble_image = (ensemble_image * 255).astype(np.uint8)\n",
    "        # add 2 other color channels\n",
    "        ensemble_image = np.stack([ensemble_image,ensemble_image,ensemble_image],axis=2)\n",
    "        #print(pred.shape)\n",
    "        # save to disk\n",
    "        plt.imsave(path+\"mask_\"+str(counter)+\".png\", ensemble_image)\n",
    "        counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir, foreground_threshold=foreground_threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "def ensemble_model_loss_f1(model_list,test_dataloader,rounding_policy=\"up\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert them to predictions using sigmoid and rounding\n",
    "    \n",
    "    # first compute all predictions with the model\n",
    "    for model in model_list:\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    num_models = len(model_list)\n",
    "    f1_acc = 0.\n",
    "    \n",
    "    for image, mask in tqdm(test_dataloader):\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        ensemble_image = torch.zeros((1,1,1024,1024),device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model in model_list:\n",
    "                pred = model(image)\n",
    "                pred = torch.round(torch.sigmoid(pred))\n",
    "                ensemble_image = ensemble_image + pred\n",
    "        \n",
    "        # now the ensemble image is the sum of all predictions\n",
    "        # need to round and find most common prediction: it should be numbers in range of 0 to num_models\n",
    "        if rounding_policy == \"up\" or rounding_policy == \"down\":\n",
    "            ensemble_image = ensemble_image/num_models\n",
    "        \n",
    "            # round the image\n",
    "            if rounding_policy == \"up\":\n",
    "                ensemble_image = torch.round(ensemble_image)\n",
    "            elif rounding_policy == \"down\":\n",
    "                ensemble_image = torch.floor(ensemble_image)\n",
    "            else:\n",
    "                raise ValueError(\"invalid rounding policy\")\n",
    "        elif rounding_policy == \"min1\":\n",
    "            ensemble_image[ensemble_image > 1] = 1\n",
    "            #ensemble_image = np.round(ensemble_image)\n",
    "        else:\n",
    "            raise ValueError(\"invalid rounding policy\")\n",
    "        \n",
    "        # compute the f1 score here: \n",
    "        \n",
    "      \n",
    "        # compute the f1 score\n",
    "        f1_acc += mean_f1_score_from_classes(ensemble_image,mask)\n",
    "        \n",
    "    print(\"mean f1_score: \",f1_acc/len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Submission for the BiSeSAM decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    # BEST LOSS SUBMISSION\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestloss\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_to_submission(model,submission_dataloader,submission_filename =  f\"decoder_{decoder_option}_bestloss_submission.csv\")\n",
    "    # BEST F1 SUBMISSION\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_to_submission(model,submission_dataloader,submission_filename =  f\"decoder_{decoder_option}_bestf1_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Vote BiSeSAM Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a majority vote ensemble of the models\n",
    "model_list = []\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_list.append(model)\n",
    "\n",
    "ensemble_models_to_submission(model_list,submission_dataloader,rounding_policy=\"up\",submission_filename =  \"majority_vote_ensemble_submission.csv\",foreground_threshold=0.25)\n",
    "\n",
    "# compute the loss on the local kaggle test set\n",
    "ensemble_model_loss_f1(model_list,kaggle_test_dataloader,rounding_policy=\"up\",foreground_threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis Confusion Matrix\n",
    "\n",
    "Now we go over each model and their their error behaviour using a confusion matrix analysis;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of trainable parameters:  0.010153446180800859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder option: mlp\n",
      "tp: 4430312.0 tn: 23165632.0 fp: 668334.0 fn: 1095850.0\n",
      "29360128.0\n",
      "tp ratio: 0.1508955274309431 tn ratio: 0.7890167236328125 fp ratio: 0.02276332037789481 fn ratio: 0.03732442855834961\n",
      "f1 score: 0.833956152431178\n",
      "accuracy: 0.9399122510637555\n",
      "precision: 0.8689193170108299\n",
      "recall: 0.8016978148668099\n",
      "Percentage of trainable parameters:  0.0019404142338121852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder option: conv\n",
      "tp: 4319282.0 tn: 23235330.0 fp: 598773.0 fn: 1206743.0\n",
      "29360128.0\n",
      "tp ratio: 0.14711386816842215 tn ratio: 0.7913906233651298 fp ratio: 0.020394086837768555 fn ratio: 0.041101421628679545\n",
      "f1 score: 0.8271254145889346\n",
      "accuracy: 0.9385044915335519\n",
      "precision: 0.8782500399039864\n",
      "recall: 0.7816254902936559\n",
      "Percentage of trainable parameters:  0.01584915626633101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder option: spatial-small\n",
      "tp: 4402030.0 tn: 23192618.0 fp: 640818.0 fn: 1124662.0\n",
      "29360128.0\n",
      "tp ratio: 0.14993224825177873 tn ratio: 0.789935861315046 fp ratio: 0.02182613100324358 fn ratio: 0.03830575942993164\n",
      "f1 score: 0.8329652946107399\n",
      "accuracy: 0.9398681095668248\n",
      "precision: 0.8729253786749075\n",
      "recall: 0.7965035865939336\n",
      "Percentage of trainable parameters:  0.02147969360218568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:12<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder option: spatial-full\n",
      "tp: 4410563.0 tn: 23178481.0 fp: 655635.0 fn: 1115449.0\n",
      "29360128.0\n",
      "tp ratio: 0.15022288049970353 tn ratio: 0.7894543579646519 fp ratio: 0.02233079501560756 fn ratio: 0.03799196652003697\n",
      "f1 score: 0.8327937229341186\n",
      "accuracy: 0.9396772384643555\n",
      "precision: 0.8705863845037245\n",
      "recall: 0.798145751402639\n",
      "Percentage of trainable parameters:  0.022877275592923414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:12<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder option: skip-connect\n",
      "tp: 4408257.0 tn: 23185325.0 fp: 649602.0 fn: 1116944.0\n",
      "29360128.0\n",
      "tp ratio: 0.15014433860778809 tn ratio: 0.7896874632154193 fp ratio: 0.022125312260219028 fn ratio: 0.03804288591657366\n",
      "f1 score: 0.8330779566590382\n",
      "accuracy: 0.9398318018232074\n",
      "precision: 0.8715658147053922\n",
      "recall: 0.7978455444426366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"]\n",
    " \n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "\n",
    "    \n",
    "    tp,tn,fp,fn = 0.,0.,0.,0.\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image,mask in tqdm(kaggle_test_dataloader):\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            pred = model(image)\n",
    "            tp_,tn_,fp_,fn_ = get_confusion_values(pred,mask)\n",
    "            \n",
    "            tp += tp_.item()\n",
    "            tn += tn_.item()\n",
    "            fp += fp_.item()\n",
    "            fn += fn_.item()\n",
    "            \n",
    "    print(\"Decoder option:\",decoder_option)\n",
    "    print(\"tp:\",tp,\"tn:\",tn,\"fp:\",fp,\"fn:\",fn)\n",
    "    # now print tp,... ratios\n",
    "    sum_all = tp+tn+fp+fn\n",
    "    print(sum_all)\n",
    "    print(\"tp ratio:\",tp/sum_all, \"tn ratio:\",tn/sum_all, \"fp ratio:\",fp/sum_all, \"fn ratio:\",fn/sum_all)\n",
    "    print(\"f1 score:\",2*tp/(2*tp+fp+fn))\n",
    "    print(\"accuracy:\",(tp+tn)/(tp+tn+fp+fn))\n",
    "    print(\"precision:\",tp/(tp+fp))\n",
    "    print(\"recall:\",tp/(tp+fn))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Ensemble of N models\n",
    "\n",
    "The idea is to concatenate the logit predicitions of all models together, use the sigmoid on them and then feed this information into an mlp, and train this for 1 epoch. This then is the final mlp prediction.:\n",
    "\n",
    "Please note that the trained ensemble model is extremely specific to the models that were used to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now we test these models submission:\n",
    "\n",
    "# create a majority vote ensemble of the models\n",
    "decoder_options = [\"mlp\" ,\"conv\",\"spatial-small\",\"spatial-full\",\"skip-connect\"] # IMPORTANT IS THE CORRECT ORDER FOR ENSEMBLE TRAIN COMPUTATION/LOADING FROM MEMORY\n",
    "model_list = []\n",
    "\n",
    "for decoder_option in decoder_options:\n",
    "    path = f\"custom_segment_anything/model_checkpoints/finetuned_kaggle/decoder_{decoder_option}_bestf1\"\n",
    "    model = load_sam_model(decoder_option,device,encoder_finetune_num_first_layers=0,encoder_finetune_num_last_layers=0,finetuned_model_name=path,sam_checkpoint_or_finetuned=\"finetuned\",load_from_compiled=True)\n",
    "    model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_models = len(model_list)\n",
    "\n",
    "######## DEFINE MODEL ########\n",
    "from custom_segment_anything.segment_anything.CUSTOM_SAM import EnsembleMLP\n",
    "\n",
    "#gc\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# enable TF32\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "ensemble_model = EnsembleMLP(num_models)\n",
    "\n",
    "######### OPTIMIZER AND LOSS FUNCTION ########\n",
    "loss_fn = combined_loss_dice\n",
    "loss_fn = torch.compile(loss_fn)\n",
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.0001)\n",
    "\n",
    "max_num_epochs=10\n",
    "####### DATA LOADER ##########\n",
    "train_loader = kaggle_train_dataloader\n",
    "eval_d_loader = kaggle_test_dataloader\n",
    "\n",
    "\n",
    "# bring all models to the device\n",
    "\n",
    "for model in model_list:\n",
    "    model = model.to(device)\n",
    "\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "##############################\n",
    "\n",
    "\n",
    "for epoch_counter in range(max_num_epochs):\n",
    "\n",
    "    # go over all models and feed the input image: \n",
    "\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    ###### TRAINING: #######\n",
    "    loss_acc = 0.0\n",
    "    f1_acc = 0.0\n",
    "    \n",
    "    step_counter = 0\n",
    "\n",
    "    for image, mask in tqdm(train_loader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        all_preds = torch.zeros((1,1,1024,1024,num_models)).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(num_models):\n",
    "                model = model_list[model_idx]\n",
    "\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "        \n",
    "        # compute the ensemble prediction\n",
    "        ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "        # compute the loss\n",
    "        #print(ensemble_pred.shape,mask.shape,ensemble_pred.min(),ensemble_pred.max(),mask.min(),mask.max())\n",
    "        loss = loss_fn(ensemble_pred,mask)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_acc += loss.item()\n",
    "        # compute the f1 score\n",
    "        f1_acc += mean_f1_score_from_logits(ensemble_pred,mask).item()\n",
    "\n",
    "    # print the loss and f1 score\n",
    "    print(f\"Epoch {epoch_counter} (train) Loss: {loss_acc/len(train_loader)}, F1-Score: {f1_acc/len(train_loader)}\")\n",
    "     \n",
    "        \n",
    "\n",
    "    torch.save(ensemble_model.state_dict(), f\"custom_segment_anything/model_checkpoints/ensemble_mlp/ensemble_model_{epoch_counter}.pth\")\n",
    "    \n",
    "    ###### TESTING: #######\n",
    "        \n",
    "    ensemble_model.eval()\n",
    "    loss_acc = 0.\n",
    "    f1_acc = 0.\n",
    "    for image, mask in tqdm(eval_d_loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        all_preds = torch.zeros((1,1,1024,1024,num_models)).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(num_models):\n",
    "                model = model_list[model_idx]\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "        \n",
    "            # compute the ensemble prediction\n",
    "            ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "            # compute the loss\n",
    "\n",
    "            loss = loss_fn(ensemble_pred,mask)\n",
    "            loss_acc += loss.item()\n",
    "            # compute the f1 score\n",
    "            f1_acc += mean_f1_score_from_logits(ensemble_pred,mask).item()\n",
    "\n",
    "    # print the loss and f1 score\n",
    "    print(f\"Epoch {epoch_counter} (test) Loss: {loss_acc/len(eval_d_loader)}, F1-Score: {f1_acc/len(eval_d_loader)}\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now creating a submission with this trained ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epoche or 3 or 9? \n",
    "\n",
    "num_models = len(model_list)\n",
    "ensemble_model = EnsembleMLP(num_models)\n",
    "ensemble_model.load_state_dict(torch.load(\"custom_segment_anything/model_checkpoints/ensemble_mlp/ensemble_model_9.pth\"))\n",
    "\n",
    "ensemble_model = ensemble_model.to(device)\n",
    "\n",
    "# do prediction using ensemble model: \n",
    "# use sigmoid and then round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mask_to_submission import make_submission\n",
    "def trained_ensemble_models_to_submission(ensemble_model,model_list,submission_dataloader,submission_filename =  \"dummy_submission.csv\",foreground_threshold=0.25):\n",
    "    # first compute all predictions with the model\n",
    "    # assuming the model outputs logits, we then convert pass these logits to the ensemble model and then convert them to predictions using sigmoid and rounding\n",
    "    import torch.nn.functional as F\n",
    "    path = \"submissions/sam/temporary_submission/\"\n",
    "\n",
    "    # first compute all predictions with the model\n",
    "    counter = 144\n",
    "    \n",
    "    for image in tqdm(submission_dataloader):\n",
    "        image = image.to(device)    \n",
    "        all_preds = torch.zeros((1,1,1024,1024,len(model_list))).to(device)\n",
    "        with torch.no_grad():\n",
    "            # compute the predictions for all models \n",
    "            for model_idx in range(len(model_list)):\n",
    "                model = model_list[model_idx]\n",
    "                pred = model(image)\n",
    "                all_preds[:,:,:,:,model_idx] = pred\n",
    "            # now all_preds contains the predictions of all models\n",
    "            # use sigmoid on all_preds\n",
    "            all_preds = torch.sigmoid(all_preds)\n",
    "            # compute the ensemble prediction\n",
    "            ensemble_pred = ensemble_model(all_preds).squeeze(4)\n",
    "            # convert to numpy\n",
    "            #ensemble_pred = ensemble_pred\n",
    "            \n",
    "            # round the image\n",
    "            ensemble_image = torch.round(torch.sigmoid(ensemble_pred))\n",
    "            #print(ensemble_image.shape)\n",
    "            #print(\"HELLO\")\n",
    "            ensemble_image = F.interpolate(ensemble_image, size=(400,400), mode='nearest')\n",
    "            ensemble_image = ensemble_image.squeeze().squeeze().to(\"cpu\").numpy()\n",
    "            #print(ensemble_image.shape)\n",
    "            # pred is now numpy vector of shape (400,400)\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "            # store as png to disk\n",
    "            ensemble_image = (ensemble_image * 255).astype(np.uint8)\n",
    "            # add 2 other color channels\n",
    "            ensemble_image = np.stack([ensemble_image,ensemble_image,ensemble_image],axis=2)\n",
    "            #print(pred.shape)\n",
    "            # save to disk\n",
    "            plt.imsave(path+\"mask_\"+str(counter)+\".png\", ensemble_image)\n",
    "            counter += 1\n",
    "        # and store to disk with name: \n",
    "    # create submission file\n",
    "    base_dir = \"submissions/sam/temporary_submission/\"    \n",
    "    make_submission(submission_filename, base_dir, foreground_threshold= foreground_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ensemble_models_to_submission(ensemble_model,model_list,submission_dataloader,submission_filename =  \"mlp_ensemble_9.csv\", foreground_threshold=0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
